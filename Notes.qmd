---
title: "Notes"
format: html
editor: visual
---

## Notes 2/6/2025 Flow Control and Looping

The following code uses the if_else statement and different functions, grepl() and str_detect() to create a new variable using mutate(). We then summarize the total number of movies and the total number of comedy movies

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/IMDB-movies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
library(tidyverse)
library(dplyr)
library(stringr)
#add comedy variable based on if genre column has comedy. COlumn says TRUE if one of the

d <- d |>
  mutate(Comedy = if_else(grepl("CoMeDy", genres, ignore.case = TRUE), TRUE, FALSE))
#looking at genres column, need to look at each cell in column and see if comedy appears,
#have new 'Comedy" variable be true or false depending on presence of 'Comedy"
#grepl() has 2 arguments, what its looking for and where to look for it, looks for that value
#anywhere in the field, as long as its these it'll take it, Having ignore.case set to true
#will remove the case sensitivity

d <- d |>
  mutate(Comedy = if_else(str_detect(genres, "Comedy"), TRUE, FALSE))
#str_detect does the same as grepl() but the arguemtns are reversed

#here we summarize (1) the total number of movies using n() which gives the total number of
#rows and (2) the total number of comedy movies with sum() which totals the number of cells
#which = TRUE
s <- d |>
  summarise(count = n(), comedyCount = sum(Comedy))

#part 3 of challenge, add new variable for ranking using case_when()
#this code takes the ratings of the movies and then assigns a rating
#based on the averageRating. It then groups the movies by genre
#and counts the total number of movies and then gets the mean runtimeMin
d <- d |>
  mutate(ranking = case_when(
    averageRating > 0 & averageRating < 3.3 ~ "low",
    averageRating >= 3.3 & averageRating < 6.7 ~ "med",
    averageRating >= 6.7 & averageRating < 10 ~ "high"
  ))
r <- d |>
  group_by(genres) |>
  summarise(cound = n(), meanRT = mean(runtimeMinutes, na.rm = TRUE))

```

## Loops

for (i in 1:....) {do this}

The loop takes the argument and does it iteratively for the specified argument in the loop

```{r}
#for loop example, prints 1 to 10 for i. also, at j prints out the square root
#of each number 1-10
for (i in 1:10){
  print(i)
  j <- sqrt(i)
  print(j)
}
#prints each of the leters in the vector for i
for (i in c("a", "e", "i", "o", "u")){
  print(i)
}
#can also have nested for loops
for (i in 1:10){
  for(j in c('a', 'b', 'c')){
    print(i)
    print(j)
  }
}

```

Now we will do a loop of the movies data set to print out the cumulative run time through. By trying to find something in a loop need to define it outside of the loop first. The below code first sets cumRT to 0, and then loops through the runtimeMinutes column for values which are not NA and add the value of each row to cumRT, giving the cumulative value. The code below it does the same thing but it takes longer

```{r}
cumRT <- 0
for (i in d$runtimeMinutes){
  if (!is.na(i)){
   cumRT <- cumRT + i 
  }
}
cumRT

cumRT <- 0
for (i in 1:nrow(d)){
  if (!is.na(d[i,]$runtimeMinutes)){
    cumRT <- cumRT + d[i,]$runtimeMinutes
  }
}
cumRT
```

# Joins

There are inner joins and outer joins. Join is merging two tables together based on a common key

Inner Joins: If we have two tables with 3 rows and matching columns, we can merge the two tables together based on shared indicies/values in those tables. Looking for common index of a specified value in 2 different tables. Using merge() and. Called inner join because makes table smaller because it is taking the intersection of the values. Think like GIS joins![](images/clipboard-2059141050.png)

Outer Joins: Left joing, right join, full join. Called outer join becuase you are potentially making the tables longer. Taking various kinds of unions. Works for when tables have same field but may not have same values in the fields. Left and right give direction, is A going to B or is B going to A. Full join have all rows and adds rows for NA rows which dont match up. ![](images/clipboard-3654234852.png)

Now we will code some joins. First gotta do some data wrangling

```{r}
library(tidyverse)
f1 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/papers.csv"
f2 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/creators.csv"
p <- read_csv(f1, col_names = TRUE)
c <- read_csv(f2, col_names = TRUE)
head(p)
head(c)

#but first, data wrangle
p <- p |>
  separate_wider_delim(cols = Author, 
                       delim = ";", 
                       names = c("First Author", "A2", "A3", "A4"), 
                       too_few = "align_start", too_many = "drop") |> #this function here creates new columns based on the data in the original authors column in the OG file split based on ;
  mutate(A2 = str_trim(`A2`, "both"), 
         A3 = str_trim(`A3`, "both"), 
         A4 = str_trim(`A4`, "both")) #this mutate section here trims down the white space

c <- c |>
  distinct() #this function looks in data for identical rows and just keeps 1 of them, throws out redundant rows, only keeps unique rows

head(c)

#Inner joing example
#Need to provide vector of equivilance for which to join the tables
inner <- inner_join(c, p, by = c("fullName" = "First Author"))
inner <- inner_join(p, c, by = c("First Author" = "fullName"))

#left join example
left <- left_join(c, p, by = c("fullName" = "First Author"))

#right join example
right <- right_join(p, c, by = c("First Author" = "fullName"))

#using joins, we can do the following
#finding particular pubs, sql query type
find_pubs <- tibble(fullName = c("Abbott, David H")) #creates 1 cell tibble which
#we will join to out table to filter for publications with this author
inner <- inner_join(find_pubs, p, by = c("fullName" = "First Author"))
```

Fuzz Join allow for wild card expressions, lets you join tables without perfect matches. By using the regular expression, we can generalize so that it finds what is close to the match, different methods of doing regular expressions depending on need

```{r}
library(fuzzyjoin)

find_pubs <- tibble(partialName = c("^Abbott")) #this is a regular expression which
#doesnt have to be exact to find a matching cell, the ^ is the regular expression
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))

find_pubs <- tibble(partialName = c("^Wil", "ony$"))
#the ^ special expression will find any author who's name starts with Wil and the $ finds all the names who end with ony
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))
```

# 2/11/205 Notes

## Loops pt 2: while loop

while(\<\<test\>\>) {do this}

Does the test condition while it is still true, then jumps out of it when it is false

```{r}
i <- 1
while(i <= 10){
  print(i)
  i <- i + 1
}
```

## Class 7: Joins, Functions, and the Wordle Coding Challenge

## Creating Functions

my_function \<- function(\<argument list\>){

\<\<function code\>\>

return(\<value\>)

}

Having explicit return (return()) is very important, shows that it is working. Sqrt function would have arguement being the value you wanna sqrt

The x in the function can be anything you want it to be, it represents what you the user would give to the function to work on, so x would be tabular data.

This function has 2 arguments, x (our data) and reps. reps had a default value of 2, but if the user wants to change it when they call it they can, for example for 4 reps they would write out my_print_reps(x, 4)

```{r}
#By running this function R puts it into memory so we can call it and use it later
my_prints_reps <- function(x, reps = 2){
  for(i in 1:reps){
    print(x)
  }
  for(i in 1:nrow(x)){
    print(x[i, ])
  }
  return(NULL)
}
#here we create a data frame which we will pass to the function
df <- data.frame(var1 = c(1,2))
my_prints_reps(df, 4)
```

Lets say we have a pipeline of data, We can create a function first which we can then pass the data into rather than rewriting/repurposing all the code

EX: In fall fo 2024 you made a BUNCH of homerange estimations using different spatial data files, could have made a function to do it all rather than having to adjust the code to analyze each of them 1 by 1

Here is another function. This is a filter function which filters a dataframe and tells based on the value you want to function. Uses the filter function within our function, telling it what we want it to filter for. This needs a data frame, condition, and a value as arguments, it does not have any defaults. Specifies the variable (column) you want to look at and the condition you want to be true

```{r}
my_filter <- function(x, condition, variable){
  library(tidyverse)
  x <- x |> filter(!!sym(variable) %in% condition)
  return(x)
}
df <- data.frame(rowid = c(1:5), value = c("a", "b", "c", "d", "e"))
my_filter(df, condition = c("c", "e"), variable = "value")
```

## Practicing Wordle Challenge

Steps

Programming wordle puzzle, need to give feedback of is the letter there and in correct position

First, need to create function which loads in a data set using a single argument, arguement should be name of dataset which we will use to get solution from

Use this function to create variables of valid list (scrabble list) and solution list (google)

The need to winnow down solution list to only words in valid list

Pick one of the words from this overlapping set of words as a random draw

Step 3 is to create a function which will winnow down solution list to words which are only 5 letters in length, picks a random word, and splits the word into a vector of single characters

Can add word_length argument to function to set default value (in pick_solution)

Step 4: Most challenging part, create 2 more function

play_wordle() should have 3 arguemtns associated with it, the solution variable, a list of valid guesses (all words in enligh lang) and number of guesses (6). This function needs to tell player the rules, display the letters the player has not guessed yet, use readline() to have user enter a guess.

Need to compare input to solution and provide feedback

Check if puzzle was solved, if not prompt another guess until they get the answer or hit number of guesses then provide feedback

```{r}

#Getting user to enter string of variables
guess <- readline("make a guess:" )
```

# 2/13/2025 Notes

## Class 8: Starting into Stats and Inference

Need {mosaic} and {radiant} packages

```{r}
library(mosaic)
library(radiant)
```

Stats: Big Ideas

-   Population vs sample

-   Parameter vs stat

-   Measure of location, spread and shape

We have a population out there which we may know little and we want to know more

We collect observations about individuals or processes in that population as a sample

We use stats to summarize, reduce, or describe data that has some empirical distribution

We expect (hope?) that stats based on a sample give good estimates of pop-level parameters that are likewise descriptors of the distribution of the variable of interest in the larger population

A stat is some function of the data alone, a combination based on a finite amount of data. An approximation of the population that we can use to make inferences. Need to think about if the sample is unbiased, random, and representative of the population of the whole

Summarizing aspects of stats: A measure of location (central tendency; mean, median, mode, harmonic mean) for a distribution, a measure of spread (mean deviation, mean squared deviation/variance, standard deviation) or scatter around that location, and a measure of the shape (skewness, kutrosis) of the distribution.

[SPREAD]{.underline}

population variance = a parameter = SS/N aka sum((x-mean(x)\^2)/(length(x))

Sample variance = a stat = SS/n-1 = s\^2 aka var()

pop standard deviation = a parameter = sigma, radiant::sdpop()

sample standard deviation = a stat, sd()

```{r}
#function for measuring population variance

pop <- c(10, 100, 1000, 2000)
#function for population variance
popVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x))
  return(v)
}
popVar(pop)

#function for sample variance
sampVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x)-1)
  return(v)
}
meanFunc <- function(x){
  v <- (sum(x)/length(x))
  return(x)
}
meanFunc(pop)
sampVar(pop)
```

# 2/18/2025 Notes

For thursday: look through modules 13, 14, and 15

### Stats, Big Ideas

Some stats are associated with well-defined mathematical distributions (gaussian and normal dist)

Stats are estimates of the parameters of distributions

Presumption that stats we use are good estimates of population level parameters, basis of classical stats inferences (F ration; chi squared; T tests)

Lets draw a sample from some standard distribution to see how stats compare to parameters of dist

```{r}
#Fist, plot a NORMAL distribution
library(mosaic)
mu <- 10 #for the "mean" parameter
sigma <- 2 # for the "sd" parameters
plotDist("norm", mean=mu, sd=sigma, xlab="x", ylab="Frequency") #plot the distribution

s1 <- rnorm(n = 100, mean = 10, sd = 2) #rnorm pulls a random normal value, drawing a set of 10 random numbers. This is a normal distribution pull
mean(s1)

s2 <- rpois(n = 10, lambda = 10) #this does a poissan distribution
```

### Sampling Distribution

Each time we select a sample and calc summary stats, we get slightly different results. If we repeat this sampling process multiple times, we can use the results to generate a new distribution for those particular summary stats of interest

Sample Dist is the set of possivle stats that could have been generated if the data collection process were repeated many times, along with the probabilities of these possible values.

Lets create a sample dist for the mean of samples drawn from normal dist with mean of 10 and sd of 2

```{r}
reps <- 500

samp_dist_mean <- do(reps) * mean(rnorm(n = 10, mean = 10, sd = 2)) #increasing sample size (n) makes the spread on the x-axis much more narrower
str(samp_dist_mean) #generates a sampling dist for the mean of our sample
histogram(samp_dist_mean$mean)

samp_dist_median <- do(reps) * median(rnorm(n = 10, mean = 10, sd = 2))
str(samp_dist_median)
histogram(samp_dist_median$median)
```

The mean of a sampling distribution for a particular stat should be a really good point estimate of the population value for that statistic.

How reliable or unreliable are these estimates of a population parameter based on the mean of the sampling distribution for a stat of interest? How far off is a stat that we calculate based on a sampling dist likely to be from the true population value of the parameter of interest

A stat called the standard error is one way to check this

standard error (SE) = square root of the variance of the sampling distribution = standard deviation of a sampling dist

```{r}
#this is how we can calculate the standard error, using the standard dev of a sampling distribution
sd_mean <- sd(samp_dist_mean$mean)
sd_median <- sd(samp_dist_median$median)
#again, increasing the sample size being taken out will decrease/narrow down the standard deviation
#standard error is a standard deviation of a sample dist but standard deviation is not necessarily the standard error
```

Estimate SE from a single sample: We can estimate the se associated with samples of size n from a single sample of a size n as the standard deviation of a sample divided by the square root of the sample size

```{r}
x <- rnorm(n = 10, mean = 10, sd = 2)
se <- sd(x)/sqrt(length(x))
```

These are different ways of getting at a measure of uncertainty to see how out stat reflects the population/reality of the dataset. Want to have as big a sample size as you can to reduce uncertainty.

This is an example of programming a simulation. Seeing what the effects of altering something (sample size/replicates) do

### Confidence Intervals

The se can be used to derive another measure of uncertainty in a stats values: the confidence interval, or CI

The CI is thus another way of describing a stat's sampling distribution, and it plays a central role in basic inferential stats

The CI is an interval around our estimate of mean of the sampling dist for a particular stat (usually mean) and it gives us a range of values into which subsequent estimates of a stat would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated

Intuitively, higher confidence is associated with a wider interval. The 95% CI around a stat described the range of values into which a new estimate of the stat derived from a subsequent sample would be expected to fall 95% of the time. Next exercise will be generating CI calculated from repeated draws from a data set.

3rd SE method (almost never) if pop variance/standard dev is known

EXERCISE:

```{r}

x <- rnorm(n = 100, mean  = 2, sd = 4)
mean <- mean(x)
sd <- sd(x)
se <- sd/sqrt(length(x))
rep <- 1000
xRep <- do(rep) * mean(rnorm(n = 100, mean = 2, sd = 4))
seMean <- sd(xRep$mean)

```

EXERCISE: t distribution, has shape really similar to normal dist based on degrees of freedom, fat tails, short peak when below 30 degrees of freedom which is considered "low" degrees of freedom

```{r}
plotDist("t", df=99, xlab="x", ylab="Frequency", col="red") #df is degrees of freedom
plotDist("t", df = 50, add = TRUE)
plotDist("t", df = 25, add = TRUE)
plotDist("norm", mu=0, sd=1, add = TRUE)
```

Do do t distribution, do rt instead of rnorm

EXERCISE: Beta dist

```{r}
plotDist("beta", shape1 = 0.3, shape2 = 4) #shape parameters define how curve looks
reps <- 1000
s <- do(reps) * mean(rbeta(n=100, shape1 = .3, shape2 = 4)) #drawing 1000 samples of size 100 with these shape parameters
histogram(s$mean)
```
