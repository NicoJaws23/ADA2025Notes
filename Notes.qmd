---
title: "Notes"
format: html
editor: visual
---

## Notes 2/6/2025 Flow Control and Looping

The following code uses the if_else statement and different functions, grepl() and str_detect() to create a new variable using mutate(). We then summarize the total number of movies and the total number of comedy movies

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/IMDB-movies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
library(tidyverse)
library(dplyr)
library(stringr)
#add comedy variable based on if genre column has comedy. COlumn says TRUE if one of the

d <- d |>
  mutate(Comedy = if_else(grepl("CoMeDy", genres, ignore.case = TRUE), TRUE, FALSE))
#looking at genres column, need to look at each cell in column and see if comedy appears,
#have new 'Comedy" variable be true or false depending on presence of 'Comedy"
#grepl() has 2 arguments, what its looking for and where to look for it, looks for that value
#anywhere in the field, as long as its these it'll take it, Having ignore.case set to true
#will remove the case sensitivity

d <- d |>
  mutate(Comedy = if_else(str_detect(genres, "Comedy"), TRUE, FALSE))
#str_detect does the same as grepl() but the arguemtns are reversed

#here we summarize (1) the total number of movies using n() which gives the total number of
#rows and (2) the total number of comedy movies with sum() which totals the number of cells
#which = TRUE
s <- d |>
  summarise(count = n(), comedyCount = sum(Comedy))

#part 3 of challenge, add new variable for ranking using case_when()
#this code takes the ratings of the movies and then assigns a rating
#based on the averageRating. It then groups the movies by genre
#and counts the total number of movies and then gets the mean runtimeMin
d <- d |>
  mutate(ranking = case_when(
    averageRating > 0 & averageRating < 3.3 ~ "low",
    averageRating >= 3.3 & averageRating < 6.7 ~ "med",
    averageRating >= 6.7 & averageRating < 10 ~ "high"
  ))
r <- d |>
  group_by(genres) |>
  summarise(cound = n(), meanRT = mean(runtimeMinutes, na.rm = TRUE))

```

## Loops

for (i in 1:....) {do this}

The loop takes the argument and does it iteratively for the specified argument in the loop

```{r}
#for loop example, prints 1 to 10 for i. also, at j prints out the square root
#of each number 1-10
for (i in 1:10){
  print(i)
  j <- sqrt(i)
  print(j)
}
#prints each of the leters in the vector for i
for (i in c("a", "e", "i", "o", "u")){
  print(i)
}
#can also have nested for loops
for (i in 1:10){
  for(j in c('a', 'b', 'c')){
    print(i)
    print(j)
  }
}

```

Now we will do a loop of the movies data set to print out the cumulative run time through. By trying to find something in a loop need to define it outside of the loop first. The below code first sets cumRT to 0, and then loops through the runtimeMinutes column for values which are not NA and add the value of each row to cumRT, giving the cumulative value. The code below it does the same thing but it takes longer

```{r}
cumRT <- 0
for (i in d$runtimeMinutes){
  if (!is.na(i)){
   cumRT <- cumRT + i 
  }
}
cumRT

cumRT <- 0
for (i in 1:nrow(d)){
  if (!is.na(d[i,]$runtimeMinutes)){
    cumRT <- cumRT + d[i,]$runtimeMinutes
  }
}
cumRT
```

# Joins

There are inner joins and outer joins. Join is merging two tables together based on a common key

Inner Joins: If we have two tables with 3 rows and matching columns, we can merge the two tables together based on shared indicies/values in those tables. Looking for common index of a specified value in 2 different tables. Using merge() and. Called inner join because makes table smaller because it is taking the intersection of the values. Think like GIS joins![](images/clipboard-2059141050.png)

Outer Joins: Left joing, right join, full join. Called outer join becuase you are potentially making the tables longer. Taking various kinds of unions. Works for when tables have same field but may not have same values in the fields. Left and right give direction, is A going to B or is B going to A. Full join have all rows and adds rows for NA rows which dont match up. ![](images/clipboard-3654234852.png)

Now we will code some joins. First gotta do some data wrangling

```{r}
library(tidyverse)
f1 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/papers.csv"
f2 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/creators.csv"
p <- read_csv(f1, col_names = TRUE)
c <- read_csv(f2, col_names = TRUE)
head(p)
head(c)

#but first, data wrangle
p <- p |>
  separate_wider_delim(cols = Author, 
                       delim = ";", 
                       names = c("First Author", "A2", "A3", "A4"), 
                       too_few = "align_start", too_many = "drop") |> #this function here creates new columns based on the data in the original authors column in the OG file split based on ;
  mutate(A2 = str_trim(`A2`, "both"), 
         A3 = str_trim(`A3`, "both"), 
         A4 = str_trim(`A4`, "both")) #this mutate section here trims down the white space

c <- c |>
  distinct() #this function looks in data for identical rows and just keeps 1 of them, throws out redundant rows, only keeps unique rows

head(c)

#Inner joing example
#Need to provide vector of equivilance for which to join the tables
inner <- inner_join(c, p, by = c("fullName" = "First Author"))
inner <- inner_join(p, c, by = c("First Author" = "fullName"))

#left join example
left <- left_join(c, p, by = c("fullName" = "First Author"))

#right join example
right <- right_join(p, c, by = c("First Author" = "fullName"))

#using joins, we can do the following
#finding particular pubs, sql query type
find_pubs <- tibble(fullName = c("Abbott, David H")) #creates 1 cell tibble which
#we will join to out table to filter for publications with this author
inner <- inner_join(find_pubs, p, by = c("fullName" = "First Author"))
```

Fuzz Join allow for wild card expressions, lets you join tables without perfect matches. By using the regular expression, we can generalize so that it finds what is close to the match, different methods of doing regular expressions depending on need

```{r}
library(fuzzyjoin)

find_pubs <- tibble(partialName = c("^Abbott")) #this is a regular expression which
#doesnt have to be exact to find a matching cell, the ^ is the regular expression
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))

find_pubs <- tibble(partialName = c("^Wil", "ony$"))
#the ^ special expression will find any author who's name starts with Wil and the $ finds all the names who end with ony
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))
```

# 2/11/205 Notes

## Loops pt 2: while loop

while(\<\<test\>\>) {do this}

Does the test condition while it is still true, then jumps out of it when it is false

```{r}
i <- 1
while(i <= 10){
  print(i)
  i <- i + 1
}
```

## Class 7: Joins, Functions, and the Wordle Coding Challenge

## Creating Functions

my_function \<- function(\<argument list\>){

\<\<function code\>\>

return(\<value\>)

}

Having explicit return (return()) is very important, shows that it is working. Sqrt function would have arguement being the value you wanna sqrt

The x in the function can be anything you want it to be, it represents what you the user would give to the function to work on, so x would be tabular data.

This function has 2 arguments, x (our data) and reps. reps had a default value of 2, but if the user wants to change it when they call it they can, for example for 4 reps they would write out my_print_reps(x, 4)

```{r}
#By running this function R puts it into memory so we can call it and use it later
my_prints_reps <- function(x, reps = 2){
  for(i in 1:reps){
    print(x)
  }
  for(i in 1:nrow(x)){
    print(x[i, ])
  }
  return(NULL)
}
#here we create a data frame which we will pass to the function
df <- data.frame(var1 = c(1,2))
my_prints_reps(df, 4)
```

Lets say we have a pipeline of data, We can create a function first which we can then pass the data into rather than rewriting/repurposing all the code

EX: In fall fo 2024 you made a BUNCH of homerange estimations using different spatial data files, could have made a function to do it all rather than having to adjust the code to analyze each of them 1 by 1

Here is another function. This is a filter function which filters a dataframe and tells based on the value you want to function. Uses the filter function within our function, telling it what we want it to filter for. This needs a data frame, condition, and a value as arguments, it does not have any defaults. Specifies the variable (column) you want to look at and the condition you want to be true

```{r}
my_filter <- function(x, condition, variable){
  library(tidyverse)
  x <- x |> filter(!!sym(variable) %in% condition)
  return(x)
}
df <- data.frame(rowid = c(1:5), value = c("a", "b", "c", "d", "e"))
my_filter(df, condition = c("c", "e"), variable = "value")
```

## Practicing Wordle Challenge

Steps

Programming wordle puzzle, need to give feedback of is the letter there and in correct position

First, need to create function which loads in a data set using a single argument, arguement should be name of dataset which we will use to get solution from

Use this function to create variables of valid list (scrabble list) and solution list (google)

The need to winnow down solution list to only words in valid list

Pick one of the words from this overlapping set of words as a random draw

Step 3 is to create a function which will winnow down solution list to words which are only 5 letters in length, picks a random word, and splits the word into a vector of single characters

Can add word_length argument to function to set default value (in pick_solution)

Step 4: Most challenging part, create 2 more function

play_wordle() should have 3 arguemtns associated with it, the solution variable, a list of valid guesses (all words in enligh lang) and number of guesses (6). This function needs to tell player the rules, display the letters the player has not guessed yet, use readline() to have user enter a guess.

Need to compare input to solution and provide feedback

Check if puzzle was solved, if not prompt another guess until they get the answer or hit number of guesses then provide feedback

```{r}

#Getting user to enter string of variables
guess <- readline("make a guess:" )
```

# 2/13/2025 Notes

## Class 8: Starting into Stats and Inference

Need {mosaic} and {radiant} packages

```{r}
library(mosaic)
library(radiant)
```

Stats: Big Ideas

-   Population vs sample

-   Parameter vs stat

-   Measure of location, spread and shape

We have a population out there which we may know little and we want to know more

We collect observations about individuals or processes in that population as a sample

We use stats to summarize, reduce, or describe data that has some empirical distribution

We expect (hope?) that stats based on a sample give good estimates of pop-level parameters that are likewise descriptors of the distribution of the variable of interest in the larger population

A stat is some function of the data alone, a combination based on a finite amount of data. An approximation of the population that we can use to make inferences. Need to think about if the sample is unbiased, random, and representative of the population of the whole

Summarizing aspects of stats: A measure of location (central tendency; mean, median, mode, harmonic mean) for a distribution, a measure of spread (mean deviation, mean squared deviation/variance, standard deviation) or scatter around that location, and a measure of the shape (skewness, kutrosis) of the distribution.

[SPREAD]{.underline}

population variance = a parameter = SS/N aka sum((x-mean(x)\^2)/(length(x))

Sample variance = a stat = SS/n-1 = s\^2 aka var()

pop standard deviation = a parameter = sigma, radiant::sdpop()

sample standard deviation = a stat, sd()

```{r}
#function for measuring population variance

pop <- c(10, 100, 1000, 2000)
#function for population variance
popVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x))
  return(v)
}
popVar(pop)

#function for sample variance
sampVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x)-1)
  return(v)
}
meanFunc <- function(x){
  v <- (sum(x)/length(x))
  return(x)
}
meanFunc(pop)
sampVar(pop)
```

# 2/18/2025 Notes

For thursday: look through modules 13, 14, and 15

### Stats, Big Ideas

Some stats are associated with well-defined mathematical distributions (gaussian and normal dist)

Stats are estimates of the parameters of distributions

Presumption that stats we use are good estimates of population level parameters, basis of classical stats inferences (F ration; chi squared; T tests)

Lets draw a sample from some standard distribution to see how stats compare to parameters of dist

```{r}
#Fist, plot a NORMAL distribution
library(mosaic)
mu <- 10 #for the "mean" parameter
sigma <- 2 # for the "sd" parameters
plotDist("norm", mean=mu, sd=sigma, xlab="x", ylab="Frequency") #plot the distribution

s1 <- rnorm(n = 100, mean = 10, sd = 2) #rnorm pulls a random normal value, drawing a set of 10 random numbers. This is a normal distribution pull
mean(s1)

s2 <- rpois(n = 10, lambda = 10) #this does a poissan distribution
```

### Sampling Distribution

Each time we select a sample and calc summary stats, we get slightly different results. If we repeat this sampling process multiple times, we can use the results to generate a new distribution for those particular summary stats of interest

Sample Dist is the set of possivle stats that could have been generated if the data collection process were repeated many times, along with the probabilities of these possible values.

Lets create a sample dist for the mean of samples drawn from normal dist with mean of 10 and sd of 2

```{r}
reps <- 500

samp_dist_mean <- do(reps) * mean(rnorm(n = 10, mean = 10, sd = 2)) #increasing sample size (n) makes the spread on the x-axis much more narrower
str(samp_dist_mean) #generates a sampling dist for the mean of our sample
histogram(samp_dist_mean$mean)

samp_dist_median <- do(reps) * median(rnorm(n = 10, mean = 10, sd = 2))
str(samp_dist_median)
histogram(samp_dist_median$median)
```

The mean of a sampling distribution for a particular stat should be a really good point estimate of the population value for that statistic.

How reliable or unreliable are these estimates of a population parameter based on the mean of the sampling distribution for a stat of interest? How far off is a stat that we calculate based on a sampling dist likely to be from the true population value of the parameter of interest

A stat called the standard error is one way to check this

standard error (SE) = square root of the variance of the sampling distribution = standard deviation of a sampling dist

```{r}
#this is how we can calculate the standard error, using the standard dev of a sampling distribution
sd_mean <- sd(samp_dist_mean$mean)
sd_median <- sd(samp_dist_median$median)
#again, increasing the sample size being taken out will decrease/narrow down the standard deviation
#standard error is a standard deviation of a sample dist but standard deviation is not necessarily the standard error
```

Estimate SE from a single sample: We can estimate the se associated with samples of size n from a single sample of a size n as the standard deviation of a sample divided by the square root of the sample size

```{r}
x <- rnorm(n = 10, mean = 10, sd = 2)
se <- sd(x)/sqrt(length(x))
```

These are different ways of getting at a measure of uncertainty to see how out stat reflects the population/reality of the dataset. Want to have as big a sample size as you can to reduce uncertainty.

This is an example of programming a simulation. Seeing what the effects of altering something (sample size/replicates) do

### Confidence Intervals

The se can be used to derive another measure of uncertainty in a stats values: the confidence interval, or CI

The CI is thus another way of describing a stat's sampling distribution, and it plays a central role in basic inferential stats

The CI is an interval around our estimate of mean of the sampling dist for a particular stat (usually mean) and it gives us a range of values into which subsequent estimates of a stat would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated

Intuitively, higher confidence is associated with a wider interval. The 95% CI around a stat described the range of values into which a new estimate of the stat derived from a subsequent sample would be expected to fall 95% of the time. Next exercise will be generating CI calculated from repeated draws from a data set.

3rd SE method (almost never) if pop variance/standard dev is known

EXERCISE: Draw 100 random numbers from a normal dist with mean = 2 and sd = 4

```{r}

x <- rnorm(n = 100, mean  = 2, sd = 4)
mean <- mean(x)
sd <- sd(x)
se <- sd/sqrt(length(x))
rep <- 1000
xRep <- do(rep) * mean(rnorm(n = 100, mean = 2, sd = 4))
seMean <- sd(xRep$mean)

```

EXERCISE: t distribution, has shape really similar to normal dist based on degrees of freedom, fat tails, short peak when below 30 degrees of freedom which is considered "low" degrees of freedom

```{r}
plotDist("t", df=99, xlab="x", ylab="Frequency", col="red") #df is degrees of freedom
plotDist("t", df = 50, add = TRUE)
plotDist("t", df = 25, add = TRUE)
plotDist("norm", mu=0, sd=1, add = TRUE)
```

Do do t distribution, do rt instead of rnorm

EXERCISE: Beta dist,

```{r}
plotDist("beta", shape1 = 0.3, shape2 = 4) #shape parameters define how curve looks
reps <- 1000
s <- do(reps) * mean(rbeta(n=100, shape1 = .3, shape2 = 4)) #drawing 1000 samples of size 100 with these shape parameters
histogram(s$mean)
```

# 2/20/2025 Notes, Playing with Distributions

Ex5 will be due next thursday (says tuesday rn)

Sampling Dist: Set of sample/possible stats, generate sum stat of some kind

Beta Distribution, shape1 and shape2 are the exponents/values that are in the beta distribution function

```{r}
plotDist("beta", shape1 = 0.3, shape2 = 4)
x <- rbeta(100, .3, 4)
se <- sd(x)/sqrt(length(x)) #checking standard error when not doingthe reps, i.e. you have a single sample

reps <- 500
s <- do(reps)*mean(rbeta(n=100, .3, 4))
histogram(s$mean)

sd(s$mean) #this is the standard error of s which we can caluculate using the standard deviation, gives estimate of uncertainty of the sample size (n=). Can do this method when having multiple sample stats, the reps
```

Standard error defined as standard distribution of sampling distribution, but there are diff ways to calculate it

## Distribution Functions

r\_(n = ), draws random samples of a particular size n from a given dist

p\_(q =), returns the quantile associated with a given value X

q\_(p =), returns the value of X at a given quantile through the distribution, value of the inverse cumulative density functions (cdf)

d\_(x=), returns the value of the probability density function (pdf) at the values of x

Total probability density of any function has to be 1 or 100%

We use q\_() function often to get the value of X associated with particular quantiles of a distribution, whether the dist is theoretical or empirical

```{r}
#What value of X is associated with the 0.025 and 0.975 quantiles of a standard normal dist (which would be the central 95% of a given dist)
qnorm(p = c(0.025, 0.975), mean = 0, sd = 1) #gives you upper and lower 95% confidence intervals

#What value of X is associated with the 0.025 and 0.975 quantiles of a beta distribution with a shape1=2 and shape2=4
qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 =4)

#for p, can pass it however many values you want, does not just have to be 2, for example could add in 0.50 to get the midpoint of the data
```

## Confidence Intervals

The SE can be used to derive the confidence interval, CI

The CI is another way of describing a stat's sampling dist, and it play a central role in basic inferential stats

CI: Interval around our estimate of mean of the sampling distribution for a particular stat (typically a mean) and it gives us a range of values into which subsequent estimates of a stat would be expected to fall

CI is calculated as: the value of the stat +/- some critical value \* the standard error of the stat

```{r}
#What is the theoretical 95% CI around the estimate of the mean of the following vector?
x <- c(2.9, 4.8, 8.9, -3.2, 9.1, -2.5, -0.9, -0.1, 2.8, -1.7)
m <- mean(x)
se <- sd(x)/sqrt(length(x))
ci <- m + qnorm(c(0.025, 0.975)) * se #calculating confidence interval, this gives 95% CI, but can adjust to give different % CIs
#or
ci <- m + c(qnorm(0.025), qnorm(0.975)) * se
percent_ci <- 0.95
alpha <- 1 - percent_ci/100
ci <- m + qnorm(c(alpha/2, (1 - (alpha/2)))) * se
```

## Central Limit Theorem (CLT)

The sampling dist of averages of "independently and identically distributed" (iid) random variables approaches a normal dist as sample size increases

Overarching theme of all this work. Gives us good sense of mean and dist of average events in a population even though we only observe one or a small number of samples of those events and even though we do not know what the actual population dist is

CLT states that regardless of underlying probability dist of a pop of IID continuous random variables the dist of a stat will be approximately normal, centered at the pop mean for a stat...

Variables are expected to be the sum of multiple independent processes will also have distributions that are nearly normal

As sample size increases, CI should shrink, reducing uncertainty.

For small sample sizes, we change how we calculate the critical value, we use the t distribution. DF is below 30, CI is gonna get a lil more negative and positive to capture all the values. Degrees of freedom (df) is sample size minus 1, so with 30 samples, you have 29 degrees of freedom

SO instead of quantiles from a normal dist, we use the quantiles from a t dist

## Calc CIs by Boostrapping

Alternative way to calc a CI for a given stat from the data ina single sample using a Monte Carlo sim process (Boostrapping)

Lets us approximate a sampling dist even without access to pop from which samples are drawn.

```{r}
n_boot <- 10000
boot <- vector(length=n_boot) #set up dummy variable to hold our sims
n <- length(x) #boostrap sample size will be same length of data
for (i in 1:n_boot){
  boot[[i]] <- mean(sample(x, n, replace = TRUE))
}
ci <- quantile(boot, probs = c(0.025, 0.975))

```

# 2/25/2025 Hypothesis Testing

SE: sd of sampling distribution

Confidence Intervals: How much of sampling distribution falls around an estimator/stat

## Null hypothesis significance testing

Null hypoth is interpreted as a baseline hypothesis and is the claim that is presumed to be true. Claim is typically that a particular value of a population parameter estimated by a sample stat we have calculated is consistent with a particular null expectation. Alternative hypoth is the conjecture that we are testing usuaully that the sample stat is inconsistent with a null expectation

Null hypothesis, H0, is a sample stat that shows no deviation from what is expected or neutral based on the parameter space of possible outcomes under the presumed random sampling process.

Ha, alternative hypothesis, a sample stat deviates more than expected by chance from what is expected or neutral

To do hypothesis test we need to...

-   calc a test stat based on data

-   calculate the p value associated with that test stat which is the probability of obtaining, by chance, a test stat that is as high or higher than our calculated one

-   Compare test stat to some appropriate standardized sample dist with well-known mathematical properties to yield a p value

-   Evaluate whether the p value is less than or greater than the significance level

Traditional parametric stats, we make the assumption that the sampling dist of out stat takes a well understood mathematical dist (normal dist, t dist, beta dist, etc) and we calc the test stat the summarized the location of a summary stat about our data relative to that implied theoretical sampling dist

Test stat is some summary stat divided by its standard error and measures how far away from zero a summary stat is in terms of a number of standard errors

Value of our test stat is determined by both the differnece between the original sample stat and the expected null value (Difference between the mean of our sample and the expected population mean or the difference between the mean and 0) and the standard error of the sample stat

Process:

Specify the sample stat to evaluate

Specify the test stat of interest and the form of the sampling dist for that stat

Calc the tail probability, i.e. the probability of obtaining a stat (ex mean) as or more extreme than was observed assuming that null distribution. Can be one or 2 tailed

**Look at module 15 on 1 and 2 sample tests**

### One Sample T Test Example from Mod 15

```{r}
library(tidyverse)
library(mosaic)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/woolly-weights.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

#get mean, sd, and se
m <- mean(d$weight)
s <- sd(d$weight)
se <- s/sqrt(length(d$weight))

#set up 2-tailed hypothesis, the hypothesis test stat is as follows: t = (m - 7.2)/se. 7.2 is assumed value based on null hypothesis
mu <- 7.2 #null value/expected value based on prior information
t_stat <- (m - mu)/se
t_stat #so we are not 3.3 standard errors away from the expected value

#now we need to figure out the p-value associated with the test stat
#calc 95% confidence interval
ci <- m + qnorm(c(0.025, 0.975)) * se
ci2 <- m + qt(p = c(0.025, 0.975), df = 14) * se #df = 14 because degrees of freedom = length - 1 (15 - 1 in this case)

#doing a bootstrap to resample the obs and calc the mean and CIs
n_boot <- 10000
boot <- vector()
n <- length(d$weight)
for (i in 1:n_boot){
  boot[[i]] <- mean(sample(d$weight, n, replace = TRUE))
}
hist(boot)
CI <- quantile(probs = c(0.025, 0.975), boot)

#Do these CIs include the expected mean (7.2)? NO! CI ranges from 5.99 - 6.87
#Now need to calculate proability of being in lower or upper end of distriution
#Summing the probabilities assuming test stat is drawn from t distribution
p_lower <- pt(-1 * abs(t_stat), df = length(d$weight) - 1) #chance of something as low as 3.3 standard errors from the center given t distribution
p_upper <- 1 - pt(1 * abs(t_stat), df = length(d$weight) - 1) #chance of something as high as 3.3 standard errors from the center give t distribution
p <- p_lower + p_upper #sum of the proabilities, gives significance value

#Can do all of this with t.test() function
t.test(x = d$weight, mu = mu, alternative = "two.sided") #we did all the calculations this function does just as separate units
#CI from this is just ONE estimate of confidence, versus us doing 10000 interations


```

### Coding EX: A 2-sample t test

Home range data for spider monkeys

load in as d

```{r}
library(tidyverse)
library(mosaic)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/tbs-2006-2008-ranges.csv"
d <- read.csv(f)
head(d)
stats <- d |>
  group_by(sex) |>
  summarise(mK95 = mean(kernel95), sdK95 = sd(kernel95), seK95 = sd(kernel95)/sqrt(length(kernel95))) #NOTE MAKE SURE EX5 has length specified to the column, or use n() function instead of length(kernel95)

ggplot(data = d, mapping = aes(x = sex, y = kernel95)) +
  geom_boxplot() +
  geom_point()


n_boot <- 10000

#Male boot
dM <- filter(d, sex == "M")
Mboot <- vector()
n <- length(dM$kernel95)
for (i in 1:n_boot){
  Mboot[[i]] <- mean(sample(dM$kernel95, n, replace = TRUE))
}
hist(Mboot)
dM_CI <- quantile(Mboot, probs = c(0.025, 0.975))

#Female boot
dF <- filter(d, sex == "F")
Fboot <- vector()
nf <- length(dF$kernel95)
for (i in 1:n_boot){
  Fboot[[i]] <- mean(sample(dF$kernel95, nf, replace = TRUE))
}
hist(Fboot)
dF_CI <- quantile(Fboot, probs = c(0.025, 0.975)) #CI for boot
fMean <- mean(Fboot)
fSE <- sd(dF$kernel95)/sqrt(length(dF$kernel95))
CI_F <- fMean + qnorm(c(0.025, 0.975)) * fSE

#Programming t-test
#Null hyppth: No difference in Male and Female home range
#Alt: Male and female homerange are difference
#Paired 
```

# 2/27/2025

## T tests

Paired t-test looks for difference in the means: Paired observations of the same individual

Welchs t stat is default for R

Inputs can change the P value depending on the degrees of freedom

Difference of mean from an expectation divided by a standard error

df in this case is minus 2 because we have 2 groups

We have the means for each group, the sd for each group and the number of cases

```{r}
#Programming t-test
#Null hyppth: No difference in Male and Female home range
#Alt: Male and female homerange are difference
#Paired 

MaleMean <- mean(dM$kernel95)
FemaleMean <- mean(dF$kernel95)
mu <- 0
sp2 <- (((Male -1)*(sd(dM$kernel95)^2))+((Female - 1)*(sd(dF$kernel95)^2)))/df
Male <- 9
Female <- 11
df <- Male + Female - 2

t <- (MaleMean - FemaleMean - mu)/(sqrt(sp2*((1/Male)+(1/Female))))

#OR do the t.test() function
ttest <- t.test(x = dM$kernel95, y = dF$kernel95, alternative = "two.sided", var.equal = TRUE)
```

## Two Sample Permutation Test

How do we get the p value (module 15 for parametric p value)

We have a t dist where we know its 0 centered and has certain number of degrees of freedom, are under the curve associated being to the left or right of standard errors of the distribution

### 2 sample t.test using permutation

How????

Need a test stat, null hypothesis, generate a permutation dist (similar to bootstrap dist, but break association we want to test, shuffles up our data where we break any possible association, and then calculate difference with actual data), and determine p value

We are looking at sex and homerange size, so we are going to randomize one or both of these variables and then recalculate the averages for each sex. Doing this a whole bunch of times and then recalculating

Here we are generating a null distribution from our own data

```{r}
#Permutation tet
d <- d |>
  select(id, sex, kernel95)

summary <- d |>
  group_by(sex) |>
  summarise(mean = mean(kernel95))

#This is the true difference in data between male and female hr size
obs <- filter(summary, sex == "F") |> pull(mean) - filter(summary, sex == "M") |> pull(mean)

#Permute
reps <- 10000
perm <- vector()

for(i in 1:reps){
  temp <- d #temporary dataframe to hold while we permute, maintain data integrity
  temp$sex <- sample(temp$sex) #shuffle stuff
  summary <- temp |>
    group_by(sex) |> #get means after shuffling
    summarise(mean = mean(kernel95))
  perm[[i]] <- filter(summary, sex == "F") |> pull(mean) - filter(summary, sex == "M") |> pull(mean) #pull means for each sex and check difference again
} #Perm vector is dist in differnces in homerange size after each reshuffling

#Perm dist should be centerd on 0
#now we can calculate the p value

hist(perm)

#Now we estimate the p value
p <- sum(perm < -1 * abs(obs) | perm > abs(obs))/reps
```

Here is how to do a permutation test with a function instead

```{r}
library(infer)

d <- d |> specify(formula = kernel95 ~ sex)
d <- d |> hypothesize(null = "independence")
perm <- d |> generate(reps = 10000, type = "permute")
perm <- perm |> calculate(stat = "diff in means", order = c("M", "F"))
perm
visualize(perm, bins = 20)
obs <- d |>
  specify(kernel95 ~ sex) |>
  calculate(stat = "diff in means", order = c("F", "M"))
  
```

# 3/4/2025 Permutations Cont

{infer} offers functions and standard workflow for using permutation methods for hypothesis testing, where we are dealing with means, differences between means, proportions, or differences in proportions

P-values: We got out data, have a test-stat for it, simulate data based on our real data which breaks associations to create null model, then look for whether or not observed value is within rght/left tail of test stat (specify, hypothesize, generate, calculate, visualize, Module 16!!!

```{r}
library(infer) #analogs for two-sample t-tests
library(tidyverse)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/tbs-2006-2008-ranges.csv"
d <- read_csv(f, col_names = TRUE)

#infer permutation pipeline
d <- d |>
  specify(formula = kernel95 ~ sex) #articulate relationship between 2 variables we are interested in
d <- d |>
  hypothesize(null = "independence") #declare null hypothesis we want to test, we are using independence since we are interested just in these 2 variables
perm <- d |>
  generate(reps = 10000, type = "permute") #number and type of shuffling we want to do to generate replicates, this is like when we used the sample function 
perm <- perm |>
  calculate(stat = "diff in means", order = c("M", "F")) #calc summary stat, males come first because they have the larger home range size so we have a positive value here
mean(perm$stat) #should be close to 0
visualize(perm, bins = 20) #sampling distribution for test stat

#Can also use visuzalize() and shade_p_value() to plot our observed test stat value
obs <- d |> #calcuate observed test stat
  specify(kernel95 ~ sex) |>
  calculate(stat = "diff in means", order = c("M", "F"))
visualize(perm, bins = 20) + #visualize where on plot obs falls
  shade_p_value(obs_stat = obs, direction = "both")
p <- get_p_value(perm, obs, direction = "both") #gives us 2-tailed p-value, Module 16 goes over this workflow
```

{modelr} package is another method for doing permutation based hypothesis testing

## Intro to Regression

Need {lmodel2}, {sjPlot}, {broom}, {tidyverse}, {manipulate}, {patchwork}, {infer}

```{r}
library(tidyverse)
library(broom)
library(manipulate)
library(patchwork)
library(infer)
library(lmodel2)
library(sjPlot)
library(skimr)
```

Regression is a common form of data modeling

Exploring relationship between an outcome variable (dependent/response variable) and one or more explanatory/predictor variables

Modeling response as outcome of a number of predictors

**Simple (general) linear regression**: Resp variable is cont. numberical variable (HR size), single predictor tat is either numerical or categorical (Sex)

**Multiple (general) linear regression**: Outcome is a cont. numerical variable, multiple predictors that are either numberical or categorical

**ANOVA/ANCOVA**: Focuses on categorical predictors

**Generalized Linear Regression**: Allows for binary, categorical, count variables as outcomes (ex: Poussin regression)

**Before Modeling!!**

Start with exploratory data analysis (Univaritate summary stats skim() from {skimr})

Bivariate summary stats: Covariance expresses how much 2 number variables "change together" and whether that change is + or - (measures on a sample is the product of the deviations of each of 2 variables from their respective means divided by (sample size - 1), how much 2 variable vary together; Correlation coefficient is a standardized form of the covariance, divide by the product of the SDs of the 2 variables (a scaled covariance)

Fundamental to understand covariance and correlation

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

#Weght by height relationship
plot(d$weight, d$height)

#Calculate the covariance by hand and with cov()
x <- d$weight - mean(d$weight)
y <- d$height - mean(d$height)
cov <- sum(x*y)/999 
cov(d$weight, d$height)
#calculate the correlation by hand and wth cor()
cor <- cov/(sd(d$height)*sd(d$weight))
cor(d$weight, d$height)
```

Why regression? Use 1 or more variables to predict the value of an outcome vairable y based on the information contained in a set of predicator variables x

Describe and quantify the relationship, develop and choose among different models of the relationship between the variables of interest (importants!!!), to do analyses of covariation among sets of variables to ID/explore their relative explanatory power

### Notation for Regression

Model outcome variable y as a linear function of the explanatory/predictor variables

Use of beta values which are referred to as "regression coefficients", our analysis is trying to estimate those coefficients, while minimizing according to some criterion the error term

Y as a function of some coefficient x + another constant + vector of errors/deviation

Interested in beta values as set of coefficients we are trying to estimate using regression

**Ordindary Least Squares:** Want to find coefficient that minimize the residuales

Example using {manipulate} in module 18 to show line of best fit and minimizing the sum of squares

```{r}
library(manipulate)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

d <- mutate(d, centered_height = height - mean(height))
d <- mutate(d, centered_weight = weight - mean(weight))

p1 <- ggplot(data = d, aes(x = weight, y = height)) + geom_point()
p2 <- ggplot(data = d, aes(x = centered_weight, y = centered_height)) + geom_point()

p1 + p2

slope.test <- function(beta1, data) { #plots beta value as slope of a line
    g <- ggplot(data = data, aes(x = centered_weight, y = centered_height))
    g <- g + geom_point()
    g <- g + geom_abline(intercept = 0, slope = beta1, size = 1, colour = "blue",
        alpha = 1/2)
    ols <- sum((data$centered_height - beta1 * data$centered_weight)^2)
    g <- g + ggtitle(paste("Slope = ", beta1, "\nSum of Squared Deviations = ", round(ols,
        3)))
    g
}

manipulate(slope.test(beta1, data = d), beta1 = slider(-1, 1, initial = 0, step = 0.005)) #Manipulate lets you pass in stuff such as a function which we made and then a slider which we can change, open with little gear icon, need to run in script window
```

Can solve for beta coefficients for a uni-variate regression using covariance, variance, and correlation (3 different methods). can use lm() to do predictions

```{r}
beta1 <- cor * (sd(d$height)/sd(d$weight))
beta <- mean(d$height) - (beta1*mean(d$weight)) 
m <- lm(height~weight, data = d) #does what we just did with b1 and b0
```

# 3/6/2025 Introduction to Regression pt 2

Beta coefficients

From zombies: height \~ weight is our regression, height is Y, weight is X, Beta1 refers to the slope while Beta0 refers to the y-intercept, so we are figuring out then the line of best fit for our regression

Summary output (see below) on the model object give output of what the model was and the estimates of the betas (intercept = beta0, value below intercept = beta1).

Also get adjusted R-squared value which gives measure of the amount of variance in the Y variable that is explained by variance in the X variable (the unexplained variance), how closey observed Y values compare to what is predicted (also called mean squared error), high value = less unexplained variance, low value = more unexplained vairance

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)
plot(x = d$weight, y = d$height)
m <- lm(height ~ weight, data = d)
summary(m) #lets us see summary of our lm() results
names(m) #Names of everything returned as a result of lm()
m$coefficients #Cal the 2 betas specifically
m$fitted.values #Fitted values as predicted/fitted of height (Y) for each value of weight (X)
m$residuals #Difference between predicted and actual values
hist(m$residuals)
plot(m)
#tidy() from {broom} is helpful too, lets you pull out parts of summmary object
tidy(m)
glance(m)#also from {broom}
confint(m) #calcs 95% CIs for beta coefficients for you, so you dont have to do it by hand (use to check work later)
```

How do we evaluate the significance of the slope? Not normal slope? Need to apply a transformation

### Calculate SE of the regression Coefficients

seB1 = sqrt(sum

```{r}
#SE by hand for height by weight
```

### Calc T stat and p value by Hand for regression

t = estimate/standard error

p value = 2\*pt(t, df = nrow-2)

```{r}
tB0 <- 39.6/.596
tB1 <- 0.195019/0.004104
pB1 <- pt(47.5, df = 998, lower.tail = FALSE) * 2
```

### Linear Regression Assumptions

Sample is unbiased and representative of the populations

Predictor variables are measured with no error

Residuals have expected values (mean) of zero and are normally distributed (use QQ plots, Wilks-Shapiro, or Komolgorov-Smirnoff goodness of fit tests

Relationship between the predictor variable and the response is not "nonlinear"

Variance of the residuals is contrast across the range of predictor variables

Multiple regression: Predictors are not highly correlated

### Alternative to Ordinary Least Squares Regression

Model II regression approaches, a line of best fit is chosen that minimizes in some way the direct distance of each point to the line of best fit

```{r}
#with zombies data and {lmodel2}\
library(lmodel2)
m2 <- lmodel2(height ~ weight, data = d, range.y = "relative", range.x = "relative", nperm = 1000)
m2
plot(m2)
summary(m2)

plot(x = d$weight, y = d$height) |>
betas <- tidy(m2) |> #Can repeat this method but with different regression methods in filter() to plot multiple lines onto a plot
  filter(method == "OLS") |>
  pull(estimate)
abline(betas, col = "blue")

```

Maximum likelihood estimation: Different approach, theoretically

We want to find the parameter values (beta coefficients) for our regression model that maximize the probability of observing the data predictor variable values

### Exercise

ECV as Y, other variables as X (group size, longevity, juvenile period, reproductive lifespan)

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/Street_et_al_2017.csv"
d <- read_csv(f)
par(mfrow = c(2, 2))
plot(x = d$Group_size, y = d$ECV)
plot(x = d$Longevity, y = d$ECV)
plot(x = d$Weaning, y = d$ECV)
plot(x = d$Repro_lifespan, y = d$ECV)

#b1 and b0
d <- d |> 
  filter(!is.na(ECV) & !is.na(Group_size))

ECV_sd <- sd(d$ECV) #Y
GS_sd <- sd(d$Group_size) #X

b1 <- cor(d$Group_size, d$ECV) * (ECV_sd/GS_sd)
b0 <- mean(d$ECV) - (b1*mean(d$Group_size))
print(paste("Beta 1 =",b1, "; Beta 0 =",b0))

#Confirm result
m <- lm(ECV~Group_size, data = d)
m1 <- lm(ECV~Longevity, data = d)
m2 <- lm(ECV~Weaning, data = d)
m3 <- lm(ECV~Repro_lifespan, data = d)
#Calc b1 and b0 on filtered dataset by radiation
regCof <- function(df, rad, x, y){
  group <- df |>
    filter(Taxonomic_group == rad)
  sdX <- sd(group[[x]])
  sdY <- sd(group[[y]])
  b1 <- cor(group[[x]], group[[y]], use = "complete.obs") * (sdY/sdX)
  b0 <- mean(group[[y]]) - (b1*mean(group[[x]]))
  return(c(b1, b0))
}

#Use lm() on a filtered data set
filtLM <- function(df, rad, x, y){
  group <- df |>
    filter(Taxonomic_group == rad)
  ECVlm <- lm(group[[y]] ~ group[[x]])
  return(ECVlm)
}

cattys <- regCof(d, "Catarrhini", "Group_size", "ECV")
cattysLM <- filtLM(d, "Catarrhini", "Group_size", "ECV")
cattysLM

platys <- regCof(d, "Platyrrhini", "Group_size", "ECV")
platysLM <- filtLM(d, "Platyrrhini", "Group_size", "ECV")
platysLM

strepys <- regCof(d, "Strepsirhini", "Group_size", "ECV")
strepysLM <- filtLM(d, "Strepsirhini", "Group_size", "ECV")
strepysLM
```

Comparing Beta1s give us test stats value, can use it to tell us differnce in relation between x and y for different group of primates

# 3/11/2025, Maximum Likelihood Estimation

Ex 8 due on 3/13, this week we are finishing up with content in Module 19

### MLE

Find the parameter values that maximize the probability of observing the data

Likelihood is just a PRODUCT of a set of probabilities of seeing each one of those datums (I see X Y and Z under some given model that generate the data, what is the probability of seeing that combination of values under that particular model)

Messing with Gaussian (normal) process for most of this class, what is the probability of seeing this comination of data

Using to figure out estimates for mu and sigma

MLE Example with generated data set

```{r}
library(ggplot2)

d <- tibble(val = rnorm(50, mean = 50, sd = 10))

ggplot(d) +
  geom_histogram(aes(x = val, y = after_stat(density))) + #after stat rescales to give density, not count
    stat_function(fun = function(x) dnorm(x, mean = 65, sd = 10), color = "blue", linewidth = 1) +
    stat_function(fun = function(x) dnorm(x, mean = 50, sd = 10), color = "red", linewidth = 1) #MLE is trying to go through parameter values for normal dist and determine what is the most likely set of parameters the data is drawn from

mean <- mean(d$val)
sd <- sd(d$val)

#Likelihood of seeing value of 41 if mean = 50 and sd = 10
val <- 41
mean <- 50
sd <- 10
(likelihood <- 1/sqrt(2 * pi * sd^2) * exp((-(val - mean)^2)/(2*sd^2))) #MLE written out
#OR
(likelihood <- dnorm(val, mean, sd))

#Log likelihood
nll <- -1 * log(likelihood) #natural log
(summed_nll <- sum(nll))

#seeing 99 if mean = 50 sd =10
val <- 99
mean <- 50
sd <- 10
lh <- dnorm(val, mean, sd)
```

Now gonna use MLE to figure out what the best values are for mu and sigma

PROTIP: Lower negative log likelihood is, the more likely it is to happen

```{r}
verbose_nll <- function(val, mu, sigma){
  likelihood <- 0
  for(i in 1:length(val)){
    likelihood[[i]] = dnorm(val[[i]], mean = mu, sd = sigma)
    ll <- log(likelihood)
    message(paste0(val[[i]], " ", mean, " ", sd, " ", ll[[i]]))
  }
  nll <- -1 * sum(ll)
  return(nll)
}

val <- c(70, 75, 50)
mean <- 50
sd <- 10

verbose_nll(val, mean, sd) 

simple_nll <- function(mu, sigma, verbose = FALSE){
  ll = sum(dnorm(val, mean = mu, sd = sigma, log = TRUE))
  nll <- -1 * ll
  if(verbose == TRUE){
    message(paste0("mean=", mu," sd=", sigma, " nll=", nll))
  }
  return(nll)
}

val <- c(70, 75, 50)
mean <- 50
sd <- 10
simple_nll(mean, sd)
#Still need to figure out what the best values are tho, above just calculate the likeihood of values appearing in a dataset with those parameters
#Now we want to find mu and sigma
#Now using {optim} package (which is in bbmle) so as to constrain sigma to be posotive by setting the lower bound to 0
library(bbmle)

val <- rnorm(50, mean = 50, sd = 10)

mean(val)
mle_norm <- mle2(minuslogl = simple_nll, start = list(mu = 0, sigma = 1), method = "SANN", trace = TRUE)#SNN is simulated anneadling method of optimization
```

### Permuting lm() for ex 8

```{r}
#Permuting lm data
library(tidyverse)
library(broom)
library(mosaic)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

m <- lm(data = d, height ~ weight)
tidy(m)
obs_slope <- tidy(m) |>
  filter(term == "weight") |>
  pull(estimate)

#using loop to permute
nperm <- 1000
perm <- do(nperm) * {
  d_new <- d
  d_new$weight <- sample(d_new$weight) #permute weight because we want to disassociate it with height, setting it so there is not relationship between the 2, i.e create null distribution
  m <- lm(data = d_new, height ~ weight)
  tidy(m) |>
    filter(term == "weight") |>
    pull(estimate)
}
hist(perm$result)

permSE <- sd(perm$result)
p <- sum(perm$result > abs(obs_slope) | perm$result < -1 * abs(obs_slope))/nperm
```

## 3/25/2025, Elements of Regression

Goal of regression is to partition variance in the outcome/response variable among different sources (ie explained by regression model itself versus the left-over error or residual variance)

Can separate/partition the total variation in our y variable (sum of squares of y or SSY) into that explained by our model (regression sum of squares or SSR, deviation in estimated y values from the mean) and that which is left over as "error" (error sum of squares or SSE, deviation between observed and estimated y values)

SSY = SSR + SSE

Looking at this with the zombie data

```{r}
library(tidyverse)
library(mosaic)
library(broom)
library(ggplot2)

f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)

#Simple regression model with lm()
m <- lm(data = d, height ~ weight)

#SSY = height - mean(height)
SSY <- sum((m$model$height - mean(m$model$height))^2)

#SSR = predicted height - mean height
SSR <- sum((m$fitted.values - mean(m$model$height))^2)

#SSE = height - predicted height
SSE <- sum((m$model$height - m$fitted.values)^2)

SSY2 <- SSR + SSE
```

ANOVA: Mean Square, calculating average sum of squares

Calculating variance in each of these 3 components by dividing the sum of squares by its corresponding \# of degrees of freedom, SSY DF = n-1, SSR DF = number of predictor variables (p), SSE DF = n-(p+1) (i.e. n-p-1)

```{r}
#ANOVA

#mean overall variance
MSY <- SSY/(nrow(d) - 1)

#mean variance explained by regression equations
MSR <- SSR/(1)

#mean remaining variance
MSE <- SSE/(nrow(d) - 1 - 1)

#fratio: ratio of the explained to unexplained variance, high value means that regression explains a lot of the variance, low means variance is not very well explained. This is out test statistic
fratio <- MSR/MSE


#p value - proportion of F distribution that lies between 0 and fratio, pf() tells us what proportion of our f-dist is to the right of the fratio (larger)
pval <- pf(q = fratio, df1 = 1, df2 = 998, lower.tail = FALSE) 
#OR
pval2 <- 1 - pf(q = fratio, df1 = 1, df2 = 998)
plotDist("f", df1 = 1, df2 = 998, col="orange") #plotting an f-distrubution

(rsq <- SSR/SSY)
summary(m) #looks at multiple R-squared value

(x <- anova(m))

#Practice with uncorrelated random data
new_d <- tibble(x = rnorm(1000, mean = 100, sd =25), y = rnorm(1000, mean = 10, sd = 2))
plot(new_d$x, new_d$y)
m2 <- lm(y ~ x, data = new_d)
summary(m2)

#SSY = height - mean(height)
SSY <- sum((m2$model$y - mean(m$model$y))^2)

#SSR = predicted height - mean height
SSR <- sum((m2$fitted.values - mean(m$model$y))^2)

#SSE = height - predicted height
SSE <- sum((m2$model$y - m2$fitted.values)^2)

#mean overall variance
MSY <- SSY/(nrow(new_d) - 1)

#mean variance explained by regression equations
MSR <- SSR/(1)

#mean remaining variance
MSE <- SSE/(nrow(new_d) - 1 - 1)

fratio <- MSR/MSE
pval <- pf(q = fratio, df1 = 1, df2 = 998, lower.tail = FALSE) 

anova(m2)
```

### Model Checking

4 conditions for inference:

"L"inearity of relationship between variables (no curves, results match up with line of best fit)

"I"ndependence of residuals

"N"ormality of residuals (look at histogram, should be centered at 0)

"E"quality of variance ("homoscedasticity") of the residuals, residuals show constant variance across the range of explainatory variable

L, N, and E can be evaluated by residual analysis, I depends on how data were collected

Pass model object through plot() and look at the plots to make sure they follow L, N, and E

### EX 9 Coding Challenge

```{r}
library(tidyverse)
library(car)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/KamilarAndCooperData.csv"
d <- read_csv(f)
par(mfrow = c(2,2))
plot(d$Body_mass_female_mean, d$MaxLongevity_m)

m1 <- lm(MaxLongevity_m ~ Body_mass_female_mean, data = d)
plot(m1)
car::qqPlot(m1)
m2 <- lm(MaxLongevity_m ~ log(Body_mass_female_mean), data = d)
plot(m2)
car::qqPlot(m2)
m3 <- lm(log(MaxLongevity_m) ~ log(Body_mass_female_mean), data = d)
plot(m3)
car::qqPlot(m3)


shapiro.test(m1$residuals)
shapiro.test(m2$residuals)
shapiro.test(m3$residuals)

```

### Categorical Predictors

Can use discrete/categorical explanatory variables made up of 2 or more groups that are coded as "factors".

Using AVONET dataset, Ex 10

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)

df <- d |>
  select(Species1, Family1, Order1, Beak.Width, Beak.Depth, Tarsus.Length, Wing.Length, Tail.Length, Mass, Habitat, Migration, Trophic.Level, Trophic.Niche, Min.Latitude, Max.Latitude, Centroid.Latitude, Range.Size)
glimpse(df)
```

## 3/27/2025, Regression w/ categorical predictors

\*\*Ex 10 due 4/4 by 5pm

Ex with avonet data sex

```{r}
library(tidyverse)
library(ggplot2)
library(cowplot)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)

df <- d |>
  select(Species1, Family1, Order1, Beak.Width, Beak.Depth, Tarsus.Length, Wing.Length, Tail.Length, Mass, Habitat, Migration, Trophic.Level, Trophic.Niche, Min.Latitude, Max.Latitude, Centroid.Latitude, Range.Size)
glimpse(df)

#Categorical variables: Species1, Family1, Order1, Habitat, Trophic.Level, Trophic.Niche, Migration (migration is the only numceric)

#Whether or not and which variables predict body mass

#plots, use drop_na to get rid of na observations
#trophic level in relation to log(Mass)
tlM <- ggplot(data = df |> drop_na(Trophic.Level), aes(x = Trophic.Level, y = log(Mass))) +
  geom_boxplot() +
  geom_jitter()

#Migration (as factor) in relation to log(Mass)
mm <- ggplot(data = df |> drop_na(Migration), aes(x = as.factor(Migration), y = log(Mass))) +
  geom_boxplot() +
  geom_jitter()

hm <- ggplot(data = df |> drop_na(Habitat), aes(x = Habitat, y = log(Mass))) +
  geom_boxplot() +
  geom_jitter()
plot_grid(tlM, mm, hm)
```

Running linear models

Trophic level has 4 different categories (carnivoure, herbivore, omnivore, scavenger) . Carnivore (because it is first alphabetically) is listed as the base level category, intercept is telling us average for base level category, other values are how they compare to the mean of the base level category. Based on p-values from m1, herbivores and scavengers are significantly difference from carnivores

```{r}
m1 <- lm(log(Mass) ~ Trophic.Level, data = df)
m2 <- lm(log(Mass) ~ as.factor(Migration), data = df)
#OR mutate migration to factor first
df <- df |>
  mutate(Migration = as.factor(Migration))
m2 <- lm(log(Mass) ~ Migration, data = df)
summary(m1)
summary(m2)
```

Post-hoc test: After finding significant omnibus (is there an effects of categorical effect) F stat in an ANOVA, er can use pairwie t-tests or Tukey Honest Siginificant Differences test to determine what groups means are different from one another

Doing p-value correction based on the number of comparissions being made (bonferroni)

```{r}
(pairwise.t.test(log(df$Mass), d$Trophic.Level, p.adj = "bonferroni"))
m1aov <- aov(log(Mass) ~ Trophic.Level, data = d)
(posthoc <- TukeyHSD(m1aov, which = "Trophic.Level", conf.level = 0.95))
```

Permutation approach with inference in ANOVA using {infer}

```{r}
library(infer)
original.F <- aov(aov(log(Mass) ~ Trophic.Level, data = df)) |>
                    broom::tidy()|>
                    filter(term == "Trophic.Level")
original.F #F stat is "statistic"

#Permutation distribution using {infer}
df <- df |> mutate(logMass = log(Mass))
permuted.F <- df |>
  specify(logMass ~ Trophic.Level) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "F")
visualize(permuted.F) +
  shade_p_value(obs_stat = original.F$statistic, direction = "greater")
p.value <- permuted.F |>
  get_p_value(obs_stat = original.F$statistic, direction = "greater")
```

### Multi-factor ANOVA (2 way ANOVA)

Sometime we want to look at data which is characterized by multiple variable (ex trophic level AND migration). Testing several null hypotheses at once

```{r}
library(tidyverse)
library(ggplot2)
#Still using bird data
d <- read_csv(f, col_names = TRUE)
#Makeing 2 regression equations and assigning residuals to new variables
d <- d |>
  mutate(Beak.Length_CulmenLog = log(Beak.Length_Culmen), Tarsus.LengthLog = log(Tarsus.Length))

m1 <- lm(Beak.Length_CulmenLog ~ log(Mass), data = d)

m2 <- lm(Tarsus.LengthLog ~ log(Mass), data = d)

d <- d |>
  mutate(blcRes = m1$residuals, tlRes = m2$residuals)

ggplot(data = d, mapping = aes(x = Primary.Lifestyle, y = tlRes)) +
  geom_boxplot()

ggplot(data = d, mapping = aes(x = Trophic.Niche, y = blcRes)) +
  geom_boxplot()
```

## 4/1/2025, ANOVAssssss

Multifactor ANOVA: 2 or more categorical predictors in response to a variable

Working on EX 10

### Multiple Linear Regression and ANCOVA

Install {car} and {jtools}

MLR and ANCOVA we are looking to model a response variable in terms of more than 1 predictor variable (at least one of which is continuose)

EX with zombie data

```{r}
library(car)
library(jtools)
library(tidyverse)

f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)


m <- lm(height ~ weight + age, data = d) #Multiple linear regression
summary(m)
plot(m$model$weight, residuals(m))
plot(m$model$age, residuals(m))
plot(fitted(m), residuals(m))
summary(aov(m))

#adding in third predictor
vif(m)
m2 <- lm(height ~ weight + age + gender, data = d)
summary(m2)
boxplot(residuals(m2) ~ m2$model$gender)

#Checking for colinearity can be done with vif(), want to understand if combination variables/predictrs are highly correlated or not, calculating variance inflaction facotrs, how easily each predictors is predicted by combo of other predictors. Can use vif() function in {car}. If it exceeds 5, there is high/problimatic amount of colineariy
vif(m2)
```

## 4/3/2025, Multiple Regression and ANCOVA

Using {car} and {jtools}.

vif() testing for multi-co-linearity. = (1)/(1 - R-squared of regression)

Equation for predicting mean height from the three predictor variables (using data from code chunk above). Using regression coefficients to write equations. Mean height = 0.141\*weight + 0.662\*age + 1.61\*gender(0 or 1) + 33.3

Doing some stuff with bird dataset. What is an equation we could write to predict range size?

logRangeSize = 13.25 + 0.2650(logMass) - 0.33\*1/0 - 1.31\*1/0 - 0.33\*1/0

```{r}
library(tidyverse)
library(ggplot2)
library(cowplot)
library(car)
library(skimr)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv"
a <- read_csv(f, col_names = TRUE)

a <- a |>
  filter(Order1 == "Accipitriformes") |>
  mutate(logRange = log(Range.Size), logMass = log(Mass))

skim(a)

m1 <- lm(logRange ~ logMass + as.factor(Primary.Lifestyle), data = a)
summary(m1)
```

### CIs and Prediction Intervals

PIs rep our uncertainty about actual new predicted values for the response at each value/set of values of the explanatory variables(s)

CI is based around the estimate/mean.

We can use the predict() function to determine both the CI for the predicted mean response and PI for individual responses at a given values of x

predict() takes the model object, a set of newdata, an interval type, and a confidence level as arguments

```{r}
library(car)
library(jtools)
library(tidyverse)

f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/zombies.csv"
d <- read_csv(f, col_names = TRUE)


m <- lm(height ~ weight + age + gender, data = d) #Multiple linear regression
summary(m)

h <- 31.76 + (0.16*132) + (0.62*29) + (1.609*1)
#Need to figure out confidence interval of this, can use predict
ci <- predict(m, newdata = data.frame(age = 29, gender = "Male", weight = 132), interval = "confidence", level = 0.95) #Givs us confidence interval basd around estimate

pi <- predict(m, newdata = data.frame(age = 29, gender = "Male", weight = 132), interval = "prediction", level = 0.95) #wider interval for prediction interval
#Could pass data frame with multiple values on it
(p <- ggplot(data = d, aes(x = age, y = height)) + 
    geom_point() +
    geom_smooth(method = lm))

#Effects plots, in {jtools}
#simple modes
m <- lm(height ~ age + weight + gender, data = d)

effect_plot(m, pred = age, interval = TRUE, int.type = "confidence", int.width = 0.95, plot.points = TRUE) #accounting for variance of height in relation to predictors

#effect tools is good for plotting the relationships between predictors
#plots_summs is cool tool
plot_summs(m,
           plot.distributions = TRUE,
           rescale.distributions = TRUE,
           colors = "green") #plots regression coefficients associated with each predictors and CIs, can add multiple models to the function and all will be on the same plot. allows to visually compare multiple models
```

### Model Selection

Need to evaluate explanatory variables and alternative models in order to establish which are best able to describe the response.

Number of different algorithms to do this, forward and backward selection, F ration tests, information criteria approaches.

Different approaches may result in different parameters being included in the final model. Lots of different packages that we can use to combine results from several different models.

Can do model averaging which averages the coefficients of models to come up with better assessment of whats going on

```{r}
library(tidyverse)
library(MASS)
library(AICcmodavg)
library()
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)
d <- d |>
  mutate(logMass = log(Mass), logRS = log(Range.Size), logBeak = log(Beak.Length_Culmen), logTarsus = log(Tarsus.Length), Migration = as.factor(Migration))
#Looking at 2 or more nested models to do model eval using partial F tests
#MOdels
m1 <- lm(data = d, logBeak ~ logRS * Migration)
m2 <- lm(data = d, logBeak ~ logRS + Migration)
m3 <- lm(data = d, logBeak ~ logRS)
m4 <- lm(data = d, logBeak ~ Migration)
m5 <- lm(data = d, logBeak ~ 1)

#Do partial F ration test with anova()
anova(m2, m1, test = "F") #Testing complexity of models
```

## 4/8/2025 Model Selection and Generalized Linear Models

Working through AVONET data to mess with model selection

```{r}
library(tidyverse)
library(MASS)
library(AICcmodavg)
library(MuMIn)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/AVONETdataset1.csv"
d <- read_csv(f, col_names = TRUE)
d <- d |>
  mutate(logMass = log(Mass), logRS = log(Range.Size), logBeak = log(Beak.Length_Culmen), logTarsus = log(Tarsus.Length), Migration = as.factor(Migration))
#Looking at 2 or more nested models to do model eval using partial F tests
#MOdels
relBeak <- lm(logBeak ~ logMass, data = d)
relTarsus <- lm(logTarsus ~ logMass, data = d)
d <- d |>
  mutate(relBeak = relBeak$residuals, relTarsus = relTarsus$residuals)

m1 <- lm(data = d, logBeak ~ logRS * Migration)
m2 <- lm(data = d, logBeak ~ logRS + Migration)
m3 <- lm(data = d, logBeak ~ logRS)
m4 <- lm(data = d, logBeak ~ Migration)
m5 <- lm(data = d, logBeak ~ 1) #null model b/c there are no predictors

#Do partial F ration test with anova()
anova(m2, m1, test = "F") #Testing complexity of models
#Produces table with F-stat and p-value telling us how significant each test is
#How do we interpret the results (low p-value and high F-stat)? The interaction of the two terms adds signficant explanatory power compared to the less complext model
anova(m3, m2, test = "F") #Produces error because we are running two models on different sized data set due to differences in scoreing, need to drop nas

d_new <- d |> drop_na(logRS, Migration)
m1 <- lm(data = d_new, logBeak ~ logRS * Migration)
m2 <- lm(data = d_new, logBeak ~ logRS + Migration)
m3 <- lm(data = d_new, logBeak ~ logRS)
m4 <- lm(data = d_new, logBeak ~ Migration)
m5 <- lm(data = d_new, logBeak ~ 1)

anova(m3, m2, test = "F") #These models dont actually tell us how much explanatory power logRS adds over anything else, need to compare model with logRS to model without logRS
```

### Forward/Backward Selection

Will be exploring variables from AVONET that best predict relative beak length.

Forward selection starts with intercept model and then adds each predictro to see how they each improves the goodness-of-fit.

First we create a null model and then use the add1() function to individually test each model against the null

```{r}

d_new <- d |> drop_na(logRS, Migration, Trophic.Level, relTarsus, logTarsus, Primary.Lifestyle)

#This is the forward selection process
#To start, null model
m_null <- lm(data = d_new, relBeak ~ 1)
#Now use add1()
add1(m_null, scope = .~. + logRS + relTarsus + Migration + Trophic.Level + Primary.Lifestyle, test = "F") #How do we read the table? Primary.Lifestyle has the largest F-value and smallest unexplained variance, so it should be included in more complex model first because it adds the most explanatory power

#Now we use the update() model to add to our null model, and then we will test this new model with more predictors
m1 <-update(m_null, formula = .~. + Primary.Lifestyle)
add1(m1, scope = .~. + logRS + relTarsus + Migration + Trophic.Level, test = "F") #Based on this, Trophic.Level adds in a significant amount of explanatory power
m2 <- update(m1, formula = .~. + Trophic.Level)
add1(m2, scope = .~. + logRS + relTarsus + Migration, test = "F")

m3 <- update(m2, formula = .~. + Migration)
add1(m3, scope = .~. + logRS + relTarsus, test = "F")
#logRS and relTarsus do not add any explanatory power, m3 is the best model

##This is backwards selection
#Starts with fullest model and systematically drops terms that do not contribute to the explanatory value of the model. We use the functions drop1() and update() for this. Determining which predictors can be dropped due to least explanatory power. It is basically the same workflow, except instad of add1(), we use drop1()

```

### Shortcut Model Selection is AICC

Using {MASS} package, AICC is a way of comparing amoung different models at the same time, can do a full or a null model

```{r}
#Full model example
m_full <- lm(data = d_new, relBeak ~ logRS + relTarsus + Migration + Trophic.Level + Primary.Lifestyle)

s <- stepAIC(m_full, scope = .~., direction = "both") #If we take out minuses and add pluses, the AICC values becomes greater, which we dont want, so we keep the model with the lowest AICC, says best mode is (relBeak ~ relTarsus + Migration + Trophic.Level + Primary.Lifestyle)

#Null model
m_null <- lm(data = d_new, relBeak ~ 1)
n <- stepAIC(m_null, scope = .~. + logRS + relTarsus + Migration + Trophic.Level + Primary.Lifestyle, direction = "both")
```

### MuMIn for Model Selection

Using {MuMIn} function dredge() which explores subsets of a given global model in an automated way. First run full model, then run it through dredge(), \*\*need to set na.action in model before passing it through dredge(). Ranked in order of ascending AICC values, best model is the first one listed.

```{r}
m_full <- lm(data = d_new, relBeak ~ logRS + relTarsus + Migration + Trophic.Level + Primary.Lifestyle, na.action = na.fail)

mods <- dredge(m_full)

#Averagin models
mods.avg <- summary(model.avg(mods, subset = delta <= 4, fit = TRUE))

aictab(models, mod.names)

mods.avg$msTable
plot(mods.avg)
confint(mods.avg) #95% CIs
```

## 4/10/2025, Generalized Linear Modeling

Assignments left: ex11 (4/17), ex12 (4/24), collaborative data science project (5/5), creative data viz (5/5), extra credit (5/2)

Data science project, group of 3, have group and plan by next tuesday

option A: explore a method we havent looked at

Also need to develop our own R package, bundling data and functions all into something that can be easily distributed

option B: do something with group members data

Extending models to be more "generalized", extend simple linear regression to allow the expected value of our response variable to depend on our predictor variable(s) through what is called a link function

Good for using when we have different kinds of response variables, like binary or count data. So now we are not using an ordinary least squared method, instead we use maximum likelihood or Bayesian approaches. Trying to estimate value of regression parameters that best fit the data that we have

3 components: Linear component, error structure or random component, a link function

Linear component yields a predicted value but this value is not the predicted value of our reponse variable Y. Predicted value needs to be transformed back into a predicted Y by applying the inverse of the link function. Calculating getting an "odds ratio", the odds of getting a 0 or 1

Identity/log/logit link, use depends on the data being used/looked for

GLM is fitted with maximum likelihood process which is a iterative process. Data are taken as a given and we are trying to find most likely parameter values and a model to fit those data

### Logistic Regression

Used when response variable is a binary variable, modeling pi(subi), probability that Y equal 1 for a given value of x

Errors/residuales are not normally distributed, they have a binomial distribution

Logit transformation is the link function connecting Y to our predictors\\

EX: Titanic data

```{r}
library(tidyverse)
library(skimr)
library(ggplot2)
library(cowplot)
library(broom)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/titanic_train.csv"
d <- read_csv(f, col_names = TRUE)
d <- d |>
  select(Survived, Pclass, Name, Sex, Age, SibSp, Parch, Fare, Embarked) |>
  mutate(Pclass = as.factor(Pclass), Sex = as.factor(Sex), Embarked = as.factor(Embarked))
#survival in relation to age, sex, pclass
ageS <- ggplot(data = d, mapping = aes(x = Survived, y = Age))+
  geom_point() +
  ggtitle("Age and Survival")

sexS <- ggplot(data = d, mapping = aes(x = Sex, y = Survived))+
  geom_violin() +
  ggtitle("Sex and Survival")

classS <- ggplot(data = d, mapping = aes(x = Pclass, y = Survived))+
  geom_violin() +
  ggtitle("Class and Survival")

plot_grid(ageS, sexS, classS)

m1 <- glm(data = d, Survived ~ Sex, family = "binomial")
summary(m1) #We can use this model to build an equation
#log(odds of surviving/no surviving) = 1.056 + (-2.5*1 if Male) | (-2.5*0 if Female)
#Translated to odds: exp(log(odds of surviving/no surviving)) = exp(1.056 + (-2.5*1 if Male) | (-2.5*0 if Female))
tidy(m1)
confint(m1)

coefs <- tidy(m1) |> select(estimate)
logOR_female_survival <- coefs$estimate[1] + coefs$estimate[2] * 0 
logoR_male_survival <- coefs$estimate[1] + coefs$estimate[2] * 1

OR_female_survival <- exp(logOR_female_survival)
OR_male_survival <- exp(logoR_male_survival) #This gives you odds of surviving, ratio of probability of occurance

PR_female_survival <- OR_female_survival/(1 + OR_female_survival)
PR_male_survivial <- OR_male_survival/(1 + OR_male_survival) #Here then is the probability of survival, can be thought of as a percent


```

### Interpretation

Beta1 of logitisc regress reps change in the log(odds ratio) of the outcome for an increase in one unit of our predictor variable, X or for the change in the log(odds ratio) for being associated with a particular group relative to the baseline level (if X is categorical)

preduct() function can fit the change in the survived/did not survivie ratio by sex

```{r}
x <- data.frame(Sex = c("male", "female"))
logOR <- predict(m1, newdata = x)
OR <- exp(logOR)
y <- predict(m1, newdata = x, type = "response", se.fit = TRUE)
```

Doing above with surival with age insteead

```{r}
m <- glm(Survived ~ Age, data = d, family = "binomial")
summary(m)

tidy(m)
confint(m)

coefs <- tidy(m)
coefs$estimate[2] #pulling out age coefficient
exp(coefs$estimate[2]) #exponentiating coefficient


s <- predict(m)
g <- exp(s)

y <- predict(m, n)
```

## 4/15/2025, Generalized Linear Modeling (continued)

Challenge to see how survival is related to Pclass

```{r}
library(tidyverse)
library(skimr)
library(ggplot2)
library(cowplot)
library(broom)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/titanic_train.csv"
d <- read_csv(f, col_names = TRUE)
d <- d |>
  select(-c(PassengerId, Ticket, Cabin)) |>
  mutate(Pclass = as.factor(Pclass), Sex = as.factor(Sex), Embarked = as.factor(Embarked))

#Looking at relationship between survival and passenger class, need to add binomial for family
m <- glm(Survived ~ Pclass, data = d, family = "binomial")
summary(m) #results indicate that passenger class is significant, estimate give us log(odds ratio), log(odds ratio) = 0.5306 + (-0.6394*0 if not pclass2, 1 if pclass2) + (-1.6704* 0 if not pclass3, 1 if pclass3)

#What are the predicted odds of survival for a passenger in 1st class?
exp(0.5306)

tidy(m)
confint(m)

coefs <- tidy(m) |> select(estimate)
logOR_pclass1_survival <- coefs$estimate[1] + coefs$estimate[2] * 0 + coefs$estimate[3] * 0
logoR_pclass2_survival <- coefs$estimate[1] + coefs$estimate[2] * 1 + coefs$estimate[3] * 0
logoR_pclass3_survival <- coefs$estimate[1] + coefs$estimate[2] * 0 + coefs$estimate[3] * 1
ORclass1 <- exp(logOR_pclass1_survival)
ORclass2 <- exp(logoR_pclass2_survival)
ORclass3 <- exp(logoR_pclass3_survival)

x <- data.frame(Pclass = c("1", "2", "3"))
logOR <- predict(m, newdata = x) #get log odds
OR <- exp(logOR) #get odds
PrS_C1 <- OR[1]/(1+OR[1]) #get probability
PrS_C2 <- OR[2]/(1+OR[2])
PrS_C3 <- OR[3]/(1+OR[3])


PrS <- predict(m, newdata = x, type = "response")# gives vector of probability because of type = "response"

#Adding in sex, so now 2 predictors
m1 <- glm(Survived ~ Sex + Pclass, data = d, family = "binomial")
summary(m1)
tidy(m1)
confint(m1)

x <- data.frame(Sex = c("male"), Pclass = c("3")) #need to specify the data set, can pass a more complext data frame which has every combo of sex and class to get all of the predictions at once
logOR <- predict(m1, newdata = x) #get log odds
exp(logOR)
Prs <- predict(m1, newdata = x, type = "response")
```

### Deviance rather than variance

Measure of discrepancy used in a GLM to asses the goodness of fit of the model to the data is called the deviance, which is an analog of variance in a general linear model

Deviance is defined as 2\*(the log-liklihood of a "fully saturated" model minus the log-liklihood of the proposed model)

Deviance can be calculted functionally as -2(log-liklihood of the porposed model)

For GLMs we can use a liklihood ratio test (LRT, similar to F ratio test) to compare the ratio of likelihoods (as opposed to the ratio of explained variaance. Can use lrtest() from {lmtest} or anova() with test = "Chisq"

```{r}
library(lmtest)
lrtest(m, m1) #Comparing our 2 models, Chisq reps the deviance score between the 2 models, results suggest the explained deviance associated with adding in sex as a predictor is high
anova(m, m1, test = "Chisq") #Maybe use this for bio anth analysis comparing models
```

### Poisson Regression

Sometimes we want to model a response variable that is in the form of count data. Many discrete response variables hav counts as possible outcomes.

Binomial counts are the number of successes, x, in a fixed number of trials, n

Poisson counts are the number of occurrences of some event in a certain interval or time

Binomial counts only take values between 0 and n, Poisson counts have no upper bound

For poisson distributed varaibles, the mean and variance are equal and represented by a single parameter (lambda). We can sometimes log transform the response variables and then use a GLM with a Poisson error term. Same process as logistic regression, but family = "Poisson"

Ex with woolly data, reproductive success as a function o age

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/woollydata.csv"
d <- read_csv(f, col_names = TRUE)
p <- ggplot(data = d, aes(x = age, y = success)) +
  geom_point()
#First regression
glm <- glm(success ~ age, data = d, family = "poisson")
summary(glm)
results <- tidy(glm) #Does this model explain a significant amount of variance? Can test with liklihood ratio test
lrtest(glm)
```

## 4/17/2025, Mixed Effects Modeling

Final extension of linear regression modelling is "multilevel" or "mixed effects" modeling

Linear Mixed Models (LMM) and Generalized Linear Mixed Models (GLMM). Looking at effects of various factors and their levels on the response variable. Data has response variable and multiple measure of predictors as additional sources of variance (Tony's video game example). Modeling this with mixed effects modeling, either with fixed or random factors

Fixed factors affect all levels of interest, random effects are those that represent only a sample of the levels of interest (ID)

ANOVA and ANCOVA looks only at fixed factors, we are adding in some random factors. Making up for having not sampled across individuals equally.

Using Mixed Effects:

Broadens the scope of inferences, can use stats methods to infer something about the populationn from which the levels of the random factos have been drawn.

Random effects naturally incorporates dependence in the model and helps us account for pseudo-replication in dataset. Observations that share the same level of the random effects are explicityl modeled as being correlated. Great for dealing with tmie series data, spatially correlated data, or situation where we have repeated observations/measures from the same subjects or sampling units.

Incorporating random factors requires the use of more sophisticated estimation and fitting methods

Addressing non-independece by estimating a different set of parameters for each level of the random factor ("subject")

Example with Chimp data, assumption that grooming received is "normalized"

```{r}
library(tidyverse)
library(ggplot2)
library(skimr)
library(cowplot)
library(lme4)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/chimpgrooming.csv"
d <- read_csv(f, col_names = TRUE)
skim(d)
names(d)

a <- ggplot(data = d, mapping = aes(x = subject, y = duration)) +
  geom_boxplot()
b <- ggplot(data = d, mapping = aes(x = reprocondition, y = duration)) +
  geom_boxplot()
c <- ggplot(data = d, mapping = aes(x = parity, y = duration, color = reprocondition)) +
  geom_boxplot()
plot_grid(a, b, c)

ggplot(data = d, mapping = aes(x = reprocondition, y = duration, color = subject)) +
  geom_boxplot()




#Random intercept model
m <- lmer(data = d, duration ~ reprocondition + parity + (1|subject))
summary(m) #Shows us the formula used, how the regression coefficints were calculates, goes over variation accounted for by random effects (subject) and fixed effects (reprocondition, parity)



#Likliehood ratio test, looking for probability of seeing the data we have actually collected given a particular model. Comparing the likelihood of two models with each other, comparing fuller model to reduced model

fuller <- lmer(data = d, duration ~ reprocondition + parity + (1|subject), REML = FALSE)

reduced <- lmer(data = d, duration ~ parity + (1|subject), REML = FALSE)

anova(reduced, fuller, test = "Chisq") #result of this anova shows that differnce in likeihood between these two shows that adding reprocondition is a stronger model, had more predictive power, lower AIC of fuller model indicates it is a better model


#Random slopes mode: how relationsip between the response variable and predictor variables vary from subject to subject. Notation of (1 + fixed|random) tells model to estimate differing baseline levels of the response as well as differing responses to the fixed factor in question. isSingular warning indicates that the effects dont explain a lot of varience
m3 <- lmer(data = d, duration ~ reprocondition + parity + (1+ reprocondition|subject) + (1+parity|subject), REML = FALSE)#how everything can vary by each subject
summary(m3)
#Can use LRT for inferences
full <- lmer(data = d, duration ~ reprocondition + parity + (1+ reprocondition|subject) + (1+parity|subject), REML = FALSE)
minusRC <- lmer(data = d, duration ~ parity + (1+ reprocondition|subject) + (1+parity|subject), REML = FALSE)
minutP <- lmer(data = d, duration ~ reprocondition + (1+ reprocondition|subject) + (1+parity|subject), REML = FALSE)

anova(minusRC, full, test = "Chisq")
anova(minutP, full, test = "Chisq")

#comparing models using AICcmodavg
null <- lmer(data = d, duration ~ (1+reprocondition|subject) + (1+parity|subject), REML = FALSE)
library(AICcmodavg)
(aic_table <- aictab(list(full, minusRC, minutP, null), modnames = c("full", "minusRC", "minutP", "null")))
```

DO CHALLENGE ON OTTER BEHAVIOR

```{r}
library(tidyverse)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/Bowden-ParryOtterdata.csv"
d <- read_csv(f, col_names = TRUE)
```

## 4/22/2025, Mixed Effects Models Cont and Making Packages

Generalized, response is not continuous variable

Create new random vairable, "trial" that joins zoo and trials order

```{r}
library(tidyverse)
library(lme4)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/Bowden-ParryOtterdata.csv"
d <- read_csv(f, col_names = TRUE)

d <- d |>
  mutate(trial = paste0(zoo, trialorder)) |>
  rename(Shared = TotalSharebothharasspro, BegRec = begreceived) |>
  rowid_to_column(var = "rowid")

ggplot(data = d, mapping = aes(x = BegRec, y = Shared, color = ID)) +
  geom_point() +
  geom_jitter()

#mirroring study, begrec and offset for trial duration as fixed effects, observation-level and otterID and trial ID as random effect
#M1 the amount of begging received influence sharing frequency
#response variable is  rate, event count per time. Time must then be a predictor variable, useing offset() function to indicate this in the regression formula
#Shared ~ BegRec + offset(log(trialduration/60))
m1 <- glmer(data = d, Shared ~ BegRec + offset(log(trialduration/60)) + (1|ID) + (1|trial) + (1|rowid), family = poisson(link = log))
summary(m1)
(x <- broom.mixed::tidy(m1)) #Based on model, sharing is predicted by begging, low p-val, coefficient different from 0

```

### Building Custom Packages (misc module on course site)

Need {devtools} and {roxygen2} and {withr}

```{r}
library(tidyverse)
library(usethis)
library(devtools)
library(roxygen2)
library(withr)

#Making package for singel custom function from Module 17

#Method 1, easy way with create_packages() from {usethis}
usethis::create_package("C:\\Users\\Jawor\\Desktop\\Rpackage") #Opens new folder/project for creating  package working in this directory now. 

#Method 2: harder but more customizable
```

Build functions in separate R script files which have header information, save in R file in repo.

Making own function, need to know that nothing outside of the function will be rendered, need to make sure all code for hte function is WITHIN the function. Also need to make sure that library(), require(), or source() are not being called within the function, instead make sure the function has something like ggplot2:: in it when it calls ggplot for example.

Function documentation, comments and tags look like \#' @

Adding in dependencies with use_package() function from {usethis}, add to console which will update description file
