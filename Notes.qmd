---
title: "Notes"
format: html
editor: visual
---

## Notes 2/6/2025 Flow Control and Looping

The following code uses the if_else statement and different functions, grepl() and str_detect() to create a new variable using mutate(). We then summarize the total number of movies and the total number of comedy movies

```{r}
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/IMDB-movies.csv"
d <- read_csv(f, col_names = TRUE)
head(d)
library(tidyverse)
library(dplyr)
library(stringr)
#add comedy variable based on if genre column has comedy. COlumn says TRUE if one of the

d <- d |>
  mutate(Comedy = if_else(grepl("CoMeDy", genres, ignore.case = TRUE), TRUE, FALSE))
#looking at genres column, need to look at each cell in column and see if comedy appears,
#have new 'Comedy" variable be true or false depending on presence of 'Comedy"
#grepl() has 2 arguments, what its looking for and where to look for it, looks for that value
#anywhere in the field, as long as its these it'll take it, Having ignore.case set to true
#will remove the case sensitivity

d <- d |>
  mutate(Comedy = if_else(str_detect(genres, "Comedy"), TRUE, FALSE))
#str_detect does the same as grepl() but the arguemtns are reversed

#here we summarize (1) the total number of movies using n() which gives the total number of
#rows and (2) the total number of comedy movies with sum() which totals the number of cells
#which = TRUE
s <- d |>
  summarise(count = n(), comedyCount = sum(Comedy))

#part 3 of challenge, add new variable for ranking using case_when()
#this code takes the ratings of the movies and then assigns a rating
#based on the averageRating. It then groups the movies by genre
#and counts the total number of movies and then gets the mean runtimeMin
d <- d |>
  mutate(ranking = case_when(
    averageRating > 0 & averageRating < 3.3 ~ "low",
    averageRating >= 3.3 & averageRating < 6.7 ~ "med",
    averageRating >= 6.7 & averageRating < 10 ~ "high"
  ))
r <- d |>
  group_by(genres) |>
  summarise(cound = n(), meanRT = mean(runtimeMinutes, na.rm = TRUE))

```

## Loops

for (i in 1:....) {do this}

The loop takes the argument and does it iteratively for the specified argument in the loop

```{r}
#for loop example, prints 1 to 10 for i. also, at j prints out the square root
#of each number 1-10
for (i in 1:10){
  print(i)
  j <- sqrt(i)
  print(j)
}
#prints each of the leters in the vector for i
for (i in c("a", "e", "i", "o", "u")){
  print(i)
}
#can also have nested for loops
for (i in 1:10){
  for(j in c('a', 'b', 'c')){
    print(i)
    print(j)
  }
}

```

Now we will do a loop of the movies data set to print out the cumulative run time through. By trying to find something in a loop need to define it outside of the loop first. The below code first sets cumRT to 0, and then loops through the runtimeMinutes column for values which are not NA and add the value of each row to cumRT, giving the cumulative value. The code below it does the same thing but it takes longer

```{r}
cumRT <- 0
for (i in d$runtimeMinutes){
  if (!is.na(i)){
   cumRT <- cumRT + i 
  }
}
cumRT

cumRT <- 0
for (i in 1:nrow(d)){
  if (!is.na(d[i,]$runtimeMinutes)){
    cumRT <- cumRT + d[i,]$runtimeMinutes
  }
}
cumRT
```

# Joins

There are inner joins and outer joins. Join is merging two tables together based on a common key

Inner Joins: If we have two tables with 3 rows and matching columns, we can merge the two tables together based on shared indicies/values in those tables. Looking for common index of a specified value in 2 different tables. Using merge() and. Called inner join because makes table smaller because it is taking the intersection of the values. Think like GIS joins![](images/clipboard-2059141050.png)

Outer Joins: Left joing, right join, full join. Called outer join becuase you are potentially making the tables longer. Taking various kinds of unions. Works for when tables have same field but may not have same values in the fields. Left and right give direction, is A going to B or is B going to A. Full join have all rows and adds rows for NA rows which dont match up. ![](images/clipboard-3654234852.png)

Now we will code some joins. First gotta do some data wrangling

```{r}
library(tidyverse)
f1 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/papers.csv"
f2 <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/creators.csv"
p <- read_csv(f1, col_names = TRUE)
c <- read_csv(f2, col_names = TRUE)
head(p)
head(c)

#but first, data wrangle
p <- p |>
  separate_wider_delim(cols = Author, 
                       delim = ";", 
                       names = c("First Author", "A2", "A3", "A4"), 
                       too_few = "align_start", too_many = "drop") |> #this function here creates new columns based on the data in the original authors column in the OG file split based on ;
  mutate(A2 = str_trim(`A2`, "both"), 
         A3 = str_trim(`A3`, "both"), 
         A4 = str_trim(`A4`, "both")) #this mutate section here trims down the white space

c <- c |>
  distinct() #this function looks in data for identical rows and just keeps 1 of them, throws out redundant rows, only keeps unique rows

head(c)

#Inner joing example
#Need to provide vector of equivilance for which to join the tables
inner <- inner_join(c, p, by = c("fullName" = "First Author"))
inner <- inner_join(p, c, by = c("First Author" = "fullName"))

#left join example
left <- left_join(c, p, by = c("fullName" = "First Author"))

#right join example
right <- right_join(p, c, by = c("First Author" = "fullName"))

#using joins, we can do the following
#finding particular pubs, sql query type
find_pubs <- tibble(fullName = c("Abbott, David H")) #creates 1 cell tibble which
#we will join to out table to filter for publications with this author
inner <- inner_join(find_pubs, p, by = c("fullName" = "First Author"))
```

Fuzz Join allow for wild card expressions, lets you join tables without perfect matches. By using the regular expression, we can generalize so that it finds what is close to the match, different methods of doing regular expressions depending on need

```{r}
library(fuzzyjoin)

find_pubs <- tibble(partialName = c("^Abbott")) #this is a regular expression which
#doesnt have to be exact to find a matching cell, the ^ is the regular expression
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))

find_pubs <- tibble(partialName = c("^Wil", "ony$"))
#the ^ special expression will find any author who's name starts with Wil and the $ finds all the names who end with ony
inner_fuzzy <- regex_inner_join(p, find_pubs, by = c("First Author" = "partialName"))
```

# 2/11/205 Notes

## Loops pt 2: while loop

while(\<\<test\>\>) {do this}

Does the test condition while it is still true, then jumps out of it when it is false

```{r}
i <- 1
while(i <= 10){
  print(i)
  i <- i + 1
}
```

## Class 7: Joins, Functions, and the Wordle Coding Challenge

## Creating Functions

my_function \<- function(\<argument list\>){

\<\<function code\>\>

return(\<value\>)

}

Having explicit return (return()) is very important, shows that it is working. Sqrt function would have arguement being the value you wanna sqrt

The x in the function can be anything you want it to be, it represents what you the user would give to the function to work on, so x would be tabular data.

This function has 2 arguments, x (our data) and reps. reps had a default value of 2, but if the user wants to change it when they call it they can, for example for 4 reps they would write out my_print_reps(x, 4)

```{r}
#By running this function R puts it into memory so we can call it and use it later
my_prints_reps <- function(x, reps = 2){
  for(i in 1:reps){
    print(x)
  }
  for(i in 1:nrow(x)){
    print(x[i, ])
  }
  return(NULL)
}
#here we create a data frame which we will pass to the function
df <- data.frame(var1 = c(1,2))
my_prints_reps(df, 4)
```

Lets say we have a pipeline of data, We can create a function first which we can then pass the data into rather than rewriting/repurposing all the code

EX: In fall fo 2024 you made a BUNCH of homerange estimations using different spatial data files, could have made a function to do it all rather than having to adjust the code to analyze each of them 1 by 1

Here is another function. This is a filter function which filters a dataframe and tells based on the value you want to function. Uses the filter function within our function, telling it what we want it to filter for. This needs a data frame, condition, and a value as arguments, it does not have any defaults. Specifies the variable (column) you want to look at and the condition you want to be true

```{r}
my_filter <- function(x, condition, variable){
  library(tidyverse)
  x <- x |> filter(!!sym(variable) %in% condition)
  return(x)
}
df <- data.frame(rowid = c(1:5), value = c("a", "b", "c", "d", "e"))
my_filter(df, condition = c("c", "e"), variable = "value")
```

## Practicing Wordle Challenge

Steps

Programming wordle puzzle, need to give feedback of is the letter there and in correct position

First, need to create function which loads in a data set using a single argument, arguement should be name of dataset which we will use to get solution from

Use this function to create variables of valid list (scrabble list) and solution list (google)

The need to winnow down solution list to only words in valid list

Pick one of the words from this overlapping set of words as a random draw

Step 3 is to create a function which will winnow down solution list to words which are only 5 letters in length, picks a random word, and splits the word into a vector of single characters

Can add word_length argument to function to set default value (in pick_solution)

Step 4: Most challenging part, create 2 more function

play_wordle() should have 3 arguemtns associated with it, the solution variable, a list of valid guesses (all words in enligh lang) and number of guesses (6). This function needs to tell player the rules, display the letters the player has not guessed yet, use readline() to have user enter a guess.

Need to compare input to solution and provide feedback

Check if puzzle was solved, if not prompt another guess until they get the answer or hit number of guesses then provide feedback

```{r}

#Getting user to enter string of variables
guess <- readline("make a guess:" )
```

# 2/13/2025 Notes

## Class 8: Starting into Stats and Inference

Need {mosaic} and {radiant} packages

```{r}
library(mosaic)
library(radiant)
```

Stats: Big Ideas

-   Population vs sample

-   Parameter vs stat

-   Measure of location, spread and shape

We have a population out there which we may know little and we want to know more

We collect observations about individuals or processes in that population as a sample

We use stats to summarize, reduce, or describe data that has some empirical distribution

We expect (hope?) that stats based on a sample give good estimates of pop-level parameters that are likewise descriptors of the distribution of the variable of interest in the larger population

A stat is some function of the data alone, a combination based on a finite amount of data. An approximation of the population that we can use to make inferences. Need to think about if the sample is unbiased, random, and representative of the population of the whole

Summarizing aspects of stats: A measure of location (central tendency; mean, median, mode, harmonic mean) for a distribution, a measure of spread (mean deviation, mean squared deviation/variance, standard deviation) or scatter around that location, and a measure of the shape (skewness, kutrosis) of the distribution.

[SPREAD]{.underline}

population variance = a parameter = SS/N aka sum((x-mean(x)\^2)/(length(x))

Sample variance = a stat = SS/n-1 = s\^2 aka var()

pop standard deviation = a parameter = sigma, radiant::sdpop()

sample standard deviation = a stat, sd()

```{r}
#function for measuring population variance

pop <- c(10, 100, 1000, 2000)
#function for population variance
popVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x))
  return(v)
}
popVar(pop)

#function for sample variance
sampVar <- function(x){
  v <- sum((x-mean(x))^2)/(length(x)-1)
  return(v)
}
meanFunc <- function(x){
  v <- (sum(x)/length(x))
  return(x)
}
meanFunc(pop)
sampVar(pop)
```

# 2/18/2025 Notes

For thursday: look through modules 13, 14, and 15

### Stats, Big Ideas

Some stats are associated with well-defined mathematical distributions (gaussian and normal dist)

Stats are estimates of the parameters of distributions

Presumption that stats we use are good estimates of population level parameters, basis of classical stats inferences (F ration; chi squared; T tests)

Lets draw a sample from some standard distribution to see how stats compare to parameters of dist

```{r}
#Fist, plot a NORMAL distribution
library(mosaic)
mu <- 10 #for the "mean" parameter
sigma <- 2 # for the "sd" parameters
plotDist("norm", mean=mu, sd=sigma, xlab="x", ylab="Frequency") #plot the distribution

s1 <- rnorm(n = 100, mean = 10, sd = 2) #rnorm pulls a random normal value, drawing a set of 10 random numbers. This is a normal distribution pull
mean(s1)

s2 <- rpois(n = 10, lambda = 10) #this does a poissan distribution
```

### Sampling Distribution

Each time we select a sample and calc summary stats, we get slightly different results. If we repeat this sampling process multiple times, we can use the results to generate a new distribution for those particular summary stats of interest

Sample Dist is the set of possivle stats that could have been generated if the data collection process were repeated many times, along with the probabilities of these possible values.

Lets create a sample dist for the mean of samples drawn from normal dist with mean of 10 and sd of 2

```{r}
reps <- 500

samp_dist_mean <- do(reps) * mean(rnorm(n = 10, mean = 10, sd = 2)) #increasing sample size (n) makes the spread on the x-axis much more narrower
str(samp_dist_mean) #generates a sampling dist for the mean of our sample
histogram(samp_dist_mean$mean)

samp_dist_median <- do(reps) * median(rnorm(n = 10, mean = 10, sd = 2))
str(samp_dist_median)
histogram(samp_dist_median$median)
```

The mean of a sampling distribution for a particular stat should be a really good point estimate of the population value for that statistic.

How reliable or unreliable are these estimates of a population parameter based on the mean of the sampling distribution for a stat of interest? How far off is a stat that we calculate based on a sampling dist likely to be from the true population value of the parameter of interest

A stat called the standard error is one way to check this

standard error (SE) = square root of the variance of the sampling distribution = standard deviation of a sampling dist

```{r}
#this is how we can calculate the standard error, using the standard dev of a sampling distribution
sd_mean <- sd(samp_dist_mean$mean)
sd_median <- sd(samp_dist_median$median)
#again, increasing the sample size being taken out will decrease/narrow down the standard deviation
#standard error is a standard deviation of a sample dist but standard deviation is not necessarily the standard error
```

Estimate SE from a single sample: We can estimate the se associated with samples of size n from a single sample of a size n as the standard deviation of a sample divided by the square root of the sample size

```{r}
x <- rnorm(n = 10, mean = 10, sd = 2)
se <- sd(x)/sqrt(length(x))
```

These are different ways of getting at a measure of uncertainty to see how out stat reflects the population/reality of the dataset. Want to have as big a sample size as you can to reduce uncertainty.

This is an example of programming a simulation. Seeing what the effects of altering something (sample size/replicates) do

### Confidence Intervals

The se can be used to derive another measure of uncertainty in a stats values: the confidence interval, or CI

The CI is thus another way of describing a stat's sampling distribution, and it plays a central role in basic inferential stats

The CI is an interval around our estimate of mean of the sampling dist for a particular stat (usually mean) and it gives us a range of values into which subsequent estimates of a stat would be expected to fall some critical proportion of the time, if the sampling exercise were to be repeated

Intuitively, higher confidence is associated with a wider interval. The 95% CI around a stat described the range of values into which a new estimate of the stat derived from a subsequent sample would be expected to fall 95% of the time. Next exercise will be generating CI calculated from repeated draws from a data set.

3rd SE method (almost never) if pop variance/standard dev is known

EXERCISE: Draw 100 random numbers from a normal dist with mean = 2 and sd = 4

```{r}

x <- rnorm(n = 100, mean  = 2, sd = 4)
mean <- mean(x)
sd <- sd(x)
se <- sd/sqrt(length(x))
rep <- 1000
xRep <- do(rep) * mean(rnorm(n = 100, mean = 2, sd = 4))
seMean <- sd(xRep$mean)

```

EXERCISE: t distribution, has shape really similar to normal dist based on degrees of freedom, fat tails, short peak when below 30 degrees of freedom which is considered "low" degrees of freedom

```{r}
plotDist("t", df=99, xlab="x", ylab="Frequency", col="red") #df is degrees of freedom
plotDist("t", df = 50, add = TRUE)
plotDist("t", df = 25, add = TRUE)
plotDist("norm", mu=0, sd=1, add = TRUE)
```

Do do t distribution, do rt instead of rnorm

EXERCISE: Beta dist,

```{r}
plotDist("beta", shape1 = 0.3, shape2 = 4) #shape parameters define how curve looks
reps <- 1000
s <- do(reps) * mean(rbeta(n=100, shape1 = .3, shape2 = 4)) #drawing 1000 samples of size 100 with these shape parameters
histogram(s$mean)
```

# 2/20/2025 Notes, Playing with Distributions

Ex5 will be due next thursday (says tuesday rn)

Sampling Dist: Set of sample/possible stats, generate sum stat of some kind

Beta Distribution, shape1 and shape2 are the exponents/values that are in the beta distribution function

```{r}
plotDist("beta", shape1 = 0.3, shape2 = 4)
x <- rbeta(100, .3, 4)
se <- sd(x)/sqrt(length(x)) #checking standard error when not doingthe reps, i.e. you have a single sample

reps <- 500
s <- do(reps)*mean(rbeta(n=100, .3, 4))
histogram(s$mean)

sd(s$mean) #this is the standard error of s which we can caluculate using the standard deviation, gives estimate of uncertainty of the sample size (n=). Can do this method when having multiple sample stats, the reps
```

Standard error defined as standard distribution of sampling distribution, but there are diff ways to calculate it

## Distribution Functions

r\_(n = ), draws random samples of a particular size n from a given dist

p\_(q =), returns the quantile associated with a given value X

q\_(p =), returns the value of X at a given quantile through the distribution, value of the inverse cumulative density functions (cdf)

d\_(x=), returns the value of the probability density function (pdf) at the values of x

Total probability density of any function has to be 1 or 100%

We use q\_() function often to get the value of X associated with particular quantiles of a distribution, whether the dist is theoretical or empirical

```{r}
#What value of X is associated with the 0.025 and 0.975 quantiles of a standard normal dist (which would be the central 95% of a given dist)
qnorm(p = c(0.025, 0.975), mean = 0, sd = 1) #gives you upper and lower 95% confidence intervals

#What value of X is associated with the 0.025 and 0.975 quantiles of a beta distribution with a shape1=2 and shape2=4
qbeta(p = c(0.025, 0.975), shape1 = 2, shape2 =4)

#for p, can pass it however many values you want, does not just have to be 2, for example could add in 0.50 to get the midpoint of the data
```

## Confidence Intervals

The SE can be used to derive the confidence interval, CI

The CI is another way of describing a stat's sampling dist, and it play a central role in basic inferential stats

CI: Interval around our estimate of mean of the sampling distribution for a particular stat (typically a mean) and it gives us a range of values into which subsequent estimates of a stat would be expected to fall

CI is calculated as: the value of the stat +/- some critical value \* the standard error of the stat

```{r}
#What is the theoretical 95% CI around the estimate of the mean of the following vector?
x <- c(2.9, 4.8, 8.9, -3.2, 9.1, -2.5, -0.9, -0.1, 2.8, -1.7)
m <- mean(x)
se <- sd(x)/sqrt(length(x))
ci <- m + qnorm(c(0.025, 0.975)) * se #calculating confidence interval, this gives 95% CI, but can adjust to give different % CIs
#or
ci <- m + c(qnorm(0.025), qnorm(0.975)) * se
percent_ci <- 0.95
alpha <- 1 - percent_ci/100
ci <- m + qnorm(c(alpha/2, (1 - (alpha/2)))) * se
```

## Central Limit Theorem (CLT)

The sampling dist of averages of "independently and identically distributed" (iid) random variables approaches a normal dist as sample size increases

Overarching theme of all this work. Gives us good sense of mean and dist of average events in a population even though we only observe one or a small number of samples of those events and even though we do not know what the actual population dist is

CLT states that regardless of underlying probability dist of a pop of IID continuous random variables the dist of a stat will be approximately normal, centered at the pop mean for a stat...

Variables are expected to be the sum of multiple independent processes will also have distributions that are nearly normal

As sample size increases, CI should shrink, reducing uncertainty.

For small sample sizes, we change how we calculate the critical value, we use the t distribution. DF is below 30, CI is gonna get a lil more negative and positive to capture all the values. Degrees of freedom (df) is sample size minus 1, so with 30 samples, you have 29 degrees of freedom

SO instead of quantiles from a normal dist, we use the quantiles from a t dist

## Calc CIs by Boostrapping

Alternative way to calc a CI for a given stat from the data ina single sample using a Monte Carlo sim process (Boostrapping)

Lets us approximate a sampling dist even without access to pop from which samples are drawn.

```{r}
n_boot <- 10000
boot <- vector(length=n_boot) #set up dummy variable to hold our sims
n <- length(x) #boostrap sample size will be same length of data
for (i in 1:n_boot){
  boot[[i]] <- mean(sample(x, n, replace = TRUE))
}
ci <- quantile(boot, probs = c(0.025, 0.975))

```

# 2/25/2025 Hypothesis Testing

SE: sd of sampling distribution

Confidence Intervals: How much of sampling distribution falls around an estimator/stat

## Null hypothesis significance testing

Null hypoth is interpreted as a baseline hypothesis and is the claim that is presumed to be true. Claim is typically that a particular value of a population parameter estimated by a sample stat we have calculated is consistent with a particular null expectation. Alternative hypoth is the conjecture that we are testing usuaully that the sample stat is inconsistent with a null expectation

Null hypothesis, H0, is a sample stat that shows no deviation from what is expected or neutral based on the parameter space of possible outcomes under the presumed random sampling process.

Ha, alternative hypothesis, a sample stat deviates more than expected by chance from what is expected or neutral

To do hypothesis test we need to...

-   calc a test stat based on data

-   calculate the p value associated with that test stat which is the probability of obtaining, by chance, a test stat that is as high or higher than our calculated one

-   Compare test stat to some appropriate standardized sample dist with well-known mathematical properties to yield a p value

-   Evaluate whether the p value is less than or greater than the significance level

Traditional parametric stats, we make the assumption that the sampling dist of out stat takes a well understood mathematical dist (normal dist, t dist, beta dist, etc) and we calc the test stat the summarized the location of a summary stat about our data relative to that implied theoretical sampling dist

Test stat is some summary stat divided by its standard error and measures how far away from zero a summary stat is in terms of a number of standard errors

Value of our test stat is determined by both the differnece between the original sample stat and the expected null value (Difference between the mean of our sample and the expected population mean or the difference between the mean and 0) and the standard error of the sample stat

Process:

Specify the sample stat to evaluate

Specify the test stat of interest and the form of the sampling dist for that stat

Calc the tail probability, i.e. the probability of obtaining a stat (ex mean) as or more extreme than was observed assuming that null distribution. Can be one or 2 tailed

**Look at module 15 on 1 and 2 sample tests**

### One Sample T Test Example from Mod 15

```{r}
library(tidyverse)
library(mosaic)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/main/woolly-weights.csv"
d <- read_csv(f, col_names = TRUE)
head(d)

#get mean, sd, and se
m <- mean(d$weight)
s <- sd(d$weight)
se <- s/sqrt(length(d$weight))

#set up 2-tailed hypothesis, the hypothesis test stat is as follows: t = (m - 7.2)/se. 7.2 is assumed value based on null hypothesis
mu <- 7.2 #null value/expected value based on prior information
t_stat <- (m - mu)/se
t_stat #so we are not 3.3 standard errors away from the expected value

#now we need to figure out the p-value associated with the test stat
#calc 95% confidence interval
ci <- m + qnorm(c(0.025, 0.975)) * se
ci2 <- m + qt(p = c(0.025, 0.975), df = 14) * se #df = 14 because degrees of freedom = length - 1 (15 - 1 in this case)

#doing a bootstrap to resample the obs and calc the mean and CIs
n_boot <- 10000
boot <- vector()
n <- length(d$weight)
for (i in 1:n_boot){
  boot[[i]] <- mean(sample(d$weight, n, replace = TRUE))
}
hist(boot)
CI <- quantile(probs = c(0.025, 0.975), boot)

#Do these CIs include the expected mean (7.2)? NO! CI ranges from 5.99 - 6.87
#Now need to calculate proability of being in lower or upper end of distriution
#Summing the probabilities assuming test stat is drawn from t distribution
p_lower <- pt(-1 * abs(t_stat), df = length(d$weight) - 1) #chance of something as low as 3.3 standard errors from the center given t distribution
p_upper <- 1 - pt(1 * abs(t_stat), df = length(d$weight) - 1) #chance of something as high as 3.3 standard errors from the center give t distribution
p <- p_lower + p_upper #sum of the proabilities, gives significance value

#Can do all of this with t.test() function
t.test(x = d$weight, mu = mu, alternative = "two.sided") #we did all the calculations this function does just as separate units
#CI from this is just ONE estimate of confidence, versus us doing 10000 interations


```

### Coding EX: A 2-sample t test

Home range data for spider monkeys

load in as d

```{r}
library(tidyverse)
library(mosaic)
f <- "https://raw.githubusercontent.com/difiore/ada-datasets/refs/heads/main/tbs-2006-2008-ranges.csv"
d <- read.csv(f)
head(d)
stats <- d |>
  group_by(sex) |>
  summarise(mK95 = mean(kernel95), sdK95 = sd(kernel95), seK95 = sd(kernel95)/sqrt(length(kernel95))) #NOTE MAKE SURE EX5 has length specified to the column, or use n() function instead of length(kernel95)

ggplot(data = d, mapping = aes(x = sex, y = kernel95)) +
  geom_boxplot() +
  geom_point()


n_boot <- 10000

#Male boot
dM <- filter(d, sex == "M")
Mboot <- vector()
n <- length(dM$kernel95)
for (i in 1:n_boot){
  Mboot[[i]] <- mean(sample(dM$kernel95, n, replace = TRUE))
}
hist(Mboot)
dM_CI <- quantile(Mboot, probs = c(0.025, 0.975))

#Female boot
dF <- filter(d, sex == "F")
Fboot <- vector()
nf <- length(dF$kernel95)
for (i in 1:n_boot){
  Fboot[[i]] <- mean(sample(dF$kernel95, nf, replace = TRUE))
}
hist(Fboot)
dF_CI <- quantile(Fboot, probs = c(0.025, 0.975)) #CI for boot
fMean <- mean(Fboot)
fSE <- sd(dF$kernel95)/sqrt(length(dF$kernel95))
CI_F <- fMean + qnorm(c(0.025, 0.975)) * fSE

#Programming t-test
#Null hyppth: No difference in Male and Female home range
#Alt: Male and female homerange are difference
#Paired 
```

# 2/27/2025

## T tests

Paired t-test looks for difference in the means: Paired observations of the same individual

Welchs t stat is default for R

Inputs can change the P value depending on the degrees of freedom

Difference of mean from an expectation divided by a standard error

df in this case is minus 2 because we have 2 groups

We have the means for each group, the sd for each group and the number of cases

```{r}
#Programming t-test
#Null hyppth: No difference in Male and Female home range
#Alt: Male and female homerange are difference
#Paired 

MaleMean <- mean(dM$kernel95)
FemaleMean <- mean(dF$kernel95)
mu <- 0
sp2 <- (((Male-1)*(sd(dM$kernel95)^2))+((Female - 1)*(sd(dF$kernel95)^2)))/df
Male <- 9
Female <- 11
df <- Male + Female - 2

t <- (MaleMean - FemaleMean - mu)/(sqrt(sp2*((1/Male)+(1/Female))))

#OR do the t.test() function
ttest <- t.test(x = dM$kernel95, y = dF$kernel95, alternative = "two.sided", var.equal = TRUE)
```

## Two Sample Permutation Test

How do we get the p value (module 15 for parametric p value)

We have a t dist where we know its 0 centered and has certain number of degrees of freedom, are under the curve associated being to the left or right of standard errors of the distribution

### 2 sample t.test using permutation

How????

Need a test stat, null hypothesis, generate a permutation dist (similar to bootstrap dist, but break association we want to test, shuffles up our data where we break any possible association, and then calculate difference with actual data), and determine p value

We are looking at sex and homerange size, so we are going to randomize one or both of these variables and then recalculate the averages for each sex. Doing this a whole bunch of times and then recalculating

Here we are generating a null distribution from our own data

```{r}
#Permutation tet
d <- d |>
  select(id, sex, kernel95)

summary <- d |>
  group_by(sex) |>
  summarise(mean = mean(kernel95))

#This is the true difference in data between male and female hr size
obs <- filter(summary, sex == "F") |> pull(mean) - filter(summary, sex == "M") |> pull(mean)

#Permute
reps <- 10000
perm <- vector()

for(i in 1:reps){
  temp <- d #temporary dataframe to hold while we permute, maintain data integrity
  temp$sex <- sample(temp$sex) #shuffle stuff
  summary <- temp |>
    group_by(sex) |> #get means after shuffling
    summarise(mean = mean(kernel95))
  perm[[i]] <- filter(summary, sex == "F") |> pull(mean) - filter(summary, sex == "M") |> pull(mean) #pull means for each sex and check difference again
} #Perm vector is dist in differnces in homerange size after each reshuffling

#Perm dist should be centerd on 0
#now we can calculate the p value

hist(perm)

#Now we estimate the p value
p <- sum(perm < -1 * abs(obs) | perm > abs(obs))/reps
```

Here is how to do a permutation test with a function instead

```{r}
library(infer)

d <- d |> specify(formula = kernel95 ~ sex)
d <- d |> hypothesize(null = "independence")
perm <- d |> generate(reps = 10000, type = "permute")
perm <- perm |> calculate(stat = "diff in means", order = c("M", "F"))
perm
visualize(perm, bins = 20)
obs <- d |>
  specify(kernel95 ~ sex) |>
  calculate(stat = "diff in means", order = c("F", "M"))
  
```
